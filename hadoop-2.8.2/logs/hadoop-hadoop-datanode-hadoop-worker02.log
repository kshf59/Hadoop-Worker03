2017-12-25 11:41:58,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker02.local/192.168.28.132
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2017-12-25 11:41:58,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-12-25 11:41:59,665 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-12-25 11:41:59,845 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2017-12-25 11:41:59,846 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2017-12-25 11:41:59,857 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2017-12-25 11:41:59,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker02.local
2017-12-25 11:41:59,866 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2017-12-25 11:41:59,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2017-12-25 11:41:59,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2017-12-25 11:41:59,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2017-12-25 11:42:00,071 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-12-25 11:42:00,090 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-12-25 11:42:00,102 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2017-12-25 11:42:00,112 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-12-25 11:42:00,114 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-12-25 11:42:00,115 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-12-25 11:42:00,116 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-12-25 11:42:00,150 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 38980
2017-12-25 11:42:00,150 INFO org.mortbay.log: jetty-6.1.26
2017-12-25 11:42:00,624 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:38980
2017-12-25 11:42:01,205 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2017-12-25 11:42:01,225 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2017-12-25 11:42:01,861 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2017-12-25 11:42:01,861 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2017-12-25 11:42:02,085 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2017-12-25 11:42:02,131 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2017-12-25 11:42:02,386 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2017-12-25 11:42:02,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2017-12-25 11:42:02,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2017-12-25 11:42:02,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2017-12-25 11:42:02,601 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-12-25 11:42:02,652 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2017-12-25 11:42:03,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2017-12-25 11:42:03,518 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2017-12-25 11:42:03,527 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 1848@hadoop-worker02.local
2017-12-25 11:42:03,529 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /var/lib/hadoop2/dfs/data/1 is not formatted for namespace 1794692519. Formatting...
2017-12-25 11:42:03,530 INFO org.apache.hadoop.hdfs.server.common.Storage: Generated new storageID DS-ceff66d7-ed85-46b1-886b-6a380c086ba2 for directory /var/lib/hadoop2/dfs/data/1
2017-12-25 11:42:03,566 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 1848@hadoop-worker02.local
2017-12-25 11:42:03,567 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /var/lib/hadoop2/dfs/data/2 is not formatted for namespace 1794692519. Formatting...
2017-12-25 11:42:03,567 INFO org.apache.hadoop.hdfs.server.common.Storage: Generated new storageID DS-17803564-4596-4115-a494-de0767943c31 for directory /var/lib/hadoop2/dfs/data/2
2017-12-25 11:42:03,657 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2017-12-25 11:42:03,657 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2017-12-25 11:42:03,658 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712 is not formatted for BP-199680249-192.168.28.129-1514169681712. Formatting ...
2017-12-25 11:42:03,659 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-199680249-192.168.28.129-1514169681712 directory /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current
2017-12-25 11:42:03,750 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2017-12-25 11:42:03,750 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2017-12-25 11:42:03,751 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712 is not formatted for BP-199680249-192.168.28.129-1514169681712. Formatting ...
2017-12-25 11:42:03,751 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-199680249-192.168.28.129-1514169681712 directory /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current
2017-12-25 11:42:03,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=null
2017-12-25 11:42:03,879 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 41fec607-6e2e-4e46-b768-7baf3ef5e9ef
2017-12-25 11:42:04,015 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ceff66d7-ed85-46b1-886b-6a380c086ba2
2017-12-25 11:42:04,015 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2017-12-25 11:42:04,020 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-17803564-4596-4115-a494-de0767943c31
2017-12-25 11:42:04,020 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2017-12-25 11:42:04,026 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2017-12-25 11:42:04,040 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2017-12-25 11:42:04,041 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2017-12-25 11:42:04,057 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2017-12-25 11:42:04,044 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2017-12-25 11:42:04,145 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 89ms
2017-12-25 11:42:04,145 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 89ms
2017-12-25 11:42:04,146 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 104ms
2017-12-25 11:42:04,149 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2017-12-25 11:42:04,150 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2017-12-25 11:42:04,150 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2017-12-25 11:42:04,152 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 2ms
2017-12-25 11:42:04,150 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2017-12-25 11:42:04,152 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 3ms
2017-12-25 11:42:04,152 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 4ms
2017-12-25 11:42:04,155 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2
2017-12-25 11:42:04,156 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-17803564-4596-4115-a494-de0767943c31): finished scanning block pool BP-199680249-192.168.28.129-1514169681712
2017-12-25 11:42:04,165 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 12/25/17 1:08 PM with interval of 21600000ms
2017-12-25 11:42:04,155 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1
2017-12-25 11:42:04,191 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2017-12-25 11:42:04,195 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-ceff66d7-ed85-46b1-886b-6a380c086ba2): finished scanning block pool BP-199680249-192.168.28.129-1514169681712
2017-12-25 11:42:04,275 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-ceff66d7-ed85-46b1-886b-6a380c086ba2): no suitable block pools found to scan.  Waiting 1814399880 ms.
2017-12-25 11:42:04,284 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-17803564-4596-4115-a494-de0767943c31): no suitable block pools found to scan.  Waiting 1814399871 ms.
2017-12-25 11:42:04,434 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2017-12-25 11:42:04,434 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-12-25 11:42:04,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2fad985bcdccd8a4,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 8 msec to generate and 104 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2017-12-25 11:42:04,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2017-12-25 12:11:55,733 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker02.local/192.168.28.132"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2017-12-25 12:11:59,387 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2017-12-25 12:11:59,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker02.local/192.168.28.132
************************************************************/
2017-12-25 12:23:51,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker02.local/192.168.28.132
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2017-12-25 12:23:51,480 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-12-25 12:23:52,881 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-12-25 12:23:53,016 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2017-12-25 12:23:53,016 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2017-12-25 12:23:53,033 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2017-12-25 12:23:53,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker02.local
2017-12-25 12:23:53,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2017-12-25 12:23:53,083 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2017-12-25 12:23:53,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2017-12-25 12:23:53,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2017-12-25 12:23:53,367 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-12-25 12:23:53,387 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-12-25 12:23:53,398 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2017-12-25 12:23:53,407 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-12-25 12:23:53,410 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-12-25 12:23:53,411 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-12-25 12:23:53,411 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-12-25 12:23:53,440 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 35144
2017-12-25 12:23:53,440 INFO org.mortbay.log: jetty-6.1.26
2017-12-25 12:23:53,709 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:35144
2017-12-25 12:23:54,033 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2017-12-25 12:23:54,041 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2017-12-25 12:23:54,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2017-12-25 12:23:54,475 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2017-12-25 12:23:54,685 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2017-12-25 12:23:54,710 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2017-12-25 12:23:54,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2017-12-25 12:23:54,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2017-12-25 12:23:54,938 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2017-12-25 12:23:54,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2017-12-25 12:23:54,999 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-12-25 12:23:55,083 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2017-12-25 12:23:55,583 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2017-12-25 12:23:55,589 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2017-12-25 12:23:55,617 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 2756@hadoop-worker02.local
2017-12-25 12:23:55,659 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 2756@hadoop-worker02.local
2017-12-25 12:23:56,007 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2017-12-25 12:23:56,007 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2017-12-25 12:23:56,214 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2017-12-25 12:23:56,215 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2017-12-25 12:23:56,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=41fec607-6e2e-4e46-b768-7baf3ef5e9ef
2017-12-25 12:23:56,432 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ceff66d7-ed85-46b1-886b-6a380c086ba2
2017-12-25 12:23:56,432 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2017-12-25 12:23:56,435 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-17803564-4596-4115-a494-de0767943c31
2017-12-25 12:23:56,435 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2017-12-25 12:23:56,547 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2017-12-25 12:23:56,557 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2017-12-25 12:23:56,557 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2017-12-25 12:23:56,567 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2017-12-25 12:23:56,580 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2017-12-25 12:23:56,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 203ms
2017-12-25 12:23:56,825 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 232ms
2017-12-25 12:23:56,826 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 270ms
2017-12-25 12:23:56,830 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2017-12-25 12:23:56,831 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2017-12-25 12:23:56,831 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 0ms
2017-12-25 12:23:56,853 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2017-12-25 12:23:56,854 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2017-12-25 12:23:56,865 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 11ms
2017-12-25 12:23:56,865 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 35ms
2017-12-25 12:23:56,990 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-17803564-4596-4115-a494-de0767943c31): no suitable block pools found to scan.  Waiting 1811887166 ms.
2017-12-25 12:23:56,995 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-ceff66d7-ed85-46b1-886b-6a380c086ba2): no suitable block pools found to scan.  Waiting 1811887160 ms.
2017-12-25 12:23:57,028 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 12/25/17 1:57 PM with interval of 21600000ms
2017-12-25 12:23:57,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2017-12-25 12:23:57,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2017-12-25 12:23:57,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-12-25 12:23:57,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xd7038d05e1919538,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 5 msec to generate and 41 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2017-12-25 12:23:57,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2017-12-25 12:26:37,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741825_1001 src: /192.168.28.131:33352 dest: /192.168.28.132:50010
2017-12-25 12:26:41,919 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:33352, dest: /192.168.28.132:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-251657011_1, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741825_1001, duration: 4313326334
2017-12-25 12:26:41,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating
2017-12-25 12:26:41,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741826_1002 src: /192.168.28.131:33354 dest: /192.168.28.132:50010
2017-12-25 12:26:43,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:33354, dest: /192.168.28.132:50010, bytes: 74090689, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-251657011_1, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741826_1002, duration: 1939271980
2017-12-25 12:26:43,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741826_1002, type=LAST_IN_PIPELINE terminating
2017-12-25 12:26:44,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741827_1003 src: /192.168.28.131:33356 dest: /192.168.28.132:50010
2017-12-25 12:26:44,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:33356, dest: /192.168.28.132:50010, bytes: 65985, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-251657011_1, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741827_1003, duration: 11012376
2017-12-25 12:26:44,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741827_1003, type=LAST_IN_PIPELINE terminating
2017-12-25 12:27:49,984 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741825 for deletion
2017-12-25 12:27:49,988 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741826 for deletion
2017-12-25 12:27:49,989 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741827 for deletion
2017-12-25 12:27:49,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741826_1002 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741826
2017-12-25 12:27:49,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741825_1001 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741825
2017-12-25 12:27:49,992 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741827_1003 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741827
2017-12-25 12:43:33,259 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker02.local/192.168.28.132"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2017-12-25 12:43:37,257 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-12-25 12:43:38,097 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2017-12-25 12:43:38,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker02.local/192.168.28.132
************************************************************/
2017-12-25 20:38:40,272 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker02.local/192.168.28.132
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2017-12-25 20:38:40,285 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-12-25 20:38:41,962 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-12-25 20:38:42,154 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2017-12-25 20:38:42,154 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2017-12-25 20:38:42,162 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2017-12-25 20:38:42,164 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker02.local
2017-12-25 20:38:42,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2017-12-25 20:38:42,219 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2017-12-25 20:38:42,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2017-12-25 20:38:42,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2017-12-25 20:38:42,450 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-12-25 20:38:42,481 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-12-25 20:38:42,496 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2017-12-25 20:38:42,507 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-12-25 20:38:42,510 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-12-25 20:38:42,511 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-12-25 20:38:42,511 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-12-25 20:38:42,541 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 43711
2017-12-25 20:38:42,541 INFO org.mortbay.log: jetty-6.1.26
2017-12-25 20:38:42,904 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:43711
2017-12-25 20:38:43,474 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2017-12-25 20:38:43,484 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2017-12-25 20:38:44,107 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2017-12-25 20:38:44,107 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2017-12-25 20:38:44,335 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2017-12-25 20:38:44,392 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2017-12-25 20:38:44,587 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2017-12-25 20:38:44,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2017-12-25 20:38:44,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2017-12-25 20:38:44,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2017-12-25 20:38:44,812 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-12-25 20:38:44,850 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2017-12-25 20:38:46,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2017-12-25 20:38:46,084 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2017-12-25 20:38:46,118 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 5128@hadoop-worker02.local
2017-12-25 20:38:46,157 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 5128@hadoop-worker02.local
2017-12-25 20:38:46,289 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2017-12-25 20:38:46,290 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2017-12-25 20:38:46,835 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2017-12-25 20:38:46,835 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2017-12-25 20:38:46,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=41fec607-6e2e-4e46-b768-7baf3ef5e9ef
2017-12-25 20:38:46,960 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ceff66d7-ed85-46b1-886b-6a380c086ba2
2017-12-25 20:38:46,960 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2017-12-25 20:38:46,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-17803564-4596-4115-a494-de0767943c31
2017-12-25 20:38:46,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2017-12-25 20:38:47,031 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2017-12-25 20:38:47,046 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2017-12-25 20:38:47,046 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2017-12-25 20:38:47,047 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2017-12-25 20:38:47,056 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2017-12-25 20:38:47,123 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 66ms
2017-12-25 20:38:47,182 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 133ms
2017-12-25 20:38:47,191 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 145ms
2017-12-25 20:38:47,202 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2017-12-25 20:38:47,202 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2017-12-25 20:38:47,205 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 3ms
2017-12-25 20:38:47,208 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2017-12-25 20:38:47,208 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2017-12-25 20:38:47,212 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 3ms
2017-12-25 20:38:47,212 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 11ms
2017-12-25 20:38:47,363 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-17803564-4596-4115-a494-de0767943c31): no suitable block pools found to scan.  Waiting 1782196792 ms.
2017-12-25 20:38:47,364 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-ceff66d7-ed85-46b1-886b-6a380c086ba2): no suitable block pools found to scan.  Waiting 1782196791 ms.
2017-12-25 20:38:47,377 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 12/25/17 8:44 PM with interval of 21600000ms
2017-12-25 20:38:47,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2017-12-25 20:38:47,657 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2017-12-25 20:38:47,658 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-12-25 20:38:47,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x79785714537302f2,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 8 msec to generate and 140 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2017-12-25 20:38:47,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2017-12-25 20:40:03,732 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741828_1004 src: /192.168.28.130:34468 dest: /192.168.28.132:50010
2017-12-25 20:40:11,490 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:34468, dest: /192.168.28.132:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1316553783_1, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741828_1004, duration: 7507210066
2017-12-25 20:40:11,490 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741828_1004, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2017-12-25 20:40:11,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741829_1005 src: /192.168.28.131:42774 dest: /192.168.28.132:50010
2017-12-25 20:40:14,053 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:42774, dest: /192.168.28.132:50010, bytes: 74090689, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1316553783_1, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741829_1005, duration: 2516263084
2017-12-25 20:40:14,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741829_1005, type=LAST_IN_PIPELINE terminating
2017-12-25 20:40:14,358 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741830_1006 src: /192.168.28.130:34472 dest: /192.168.28.132:50010
2017-12-25 20:40:14,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:34472, dest: /192.168.28.132:50010, bytes: 65985, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1316553783_1, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741830_1006, duration: 13456222
2017-12-25 20:40:14,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741830_1006, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2017-12-25 20:44:36,053 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-199680249-192.168.28.129-1514169681712 Total blocks: 3, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2017-12-25 21:08:41,428 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741828_1004 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741828 for deletion
2017-12-25 21:08:41,462 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741829 for deletion
2017-12-25 21:08:41,463 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741830 for deletion
2017-12-25 21:08:41,465 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741828_1004 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741828
2017-12-25 21:08:41,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741829_1005 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741829
2017-12-25 21:08:41,481 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741830_1006 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741830
2017-12-25 21:08:44,329 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker02.local/192.168.28.132"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2017-12-25 21:08:48,124 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2017-12-25 21:08:48,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker02.local/192.168.28.132
************************************************************/
2017-12-26 22:10:52,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker02.local/192.168.28.132
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2017-12-26 22:10:52,143 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-12-26 22:10:53,880 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-12-26 22:10:54,038 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2017-12-26 22:10:54,039 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2017-12-26 22:10:54,046 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2017-12-26 22:10:54,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker02.local
2017-12-26 22:10:54,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2017-12-26 22:10:54,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2017-12-26 22:10:54,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2017-12-26 22:10:54,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2017-12-26 22:10:54,409 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-12-26 22:10:54,432 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-12-26 22:10:54,447 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2017-12-26 22:10:54,457 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-12-26 22:10:54,460 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-12-26 22:10:54,461 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-12-26 22:10:54,461 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-12-26 22:10:54,503 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 36115
2017-12-26 22:10:54,504 INFO org.mortbay.log: jetty-6.1.26
2017-12-26 22:10:57,005 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:36115
2017-12-26 22:10:57,780 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2017-12-26 22:10:57,788 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2017-12-26 22:10:58,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2017-12-26 22:10:58,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2017-12-26 22:10:58,721 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2017-12-26 22:10:58,762 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2017-12-26 22:10:58,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2017-12-26 22:10:58,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2017-12-26 22:10:59,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2017-12-26 22:10:59,092 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2017-12-26 22:10:59,166 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-12-26 22:10:59,168 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2017-12-26 22:11:00,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2017-12-26 22:11:00,350 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2017-12-26 22:11:00,397 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 5603@hadoop-worker02.local
2017-12-26 22:11:00,476 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 5603@hadoop-worker02.local
2017-12-26 22:11:00,907 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2017-12-26 22:11:00,908 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2017-12-26 22:11:01,112 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2017-12-26 22:11:01,112 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2017-12-26 22:11:01,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=41fec607-6e2e-4e46-b768-7baf3ef5e9ef
2017-12-26 22:11:01,337 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ceff66d7-ed85-46b1-886b-6a380c086ba2
2017-12-26 22:11:01,338 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2017-12-26 22:11:01,342 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-17803564-4596-4115-a494-de0767943c31
2017-12-26 22:11:01,342 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2017-12-26 22:11:01,348 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2017-12-26 22:11:01,372 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2017-12-26 22:11:01,372 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2017-12-26 22:11:01,376 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2017-12-26 22:11:01,403 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2017-12-26 22:11:01,503 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 100ms
2017-12-26 22:11:01,505 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 101ms
2017-12-26 22:11:01,505 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 133ms
2017-12-26 22:11:01,509 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2017-12-26 22:11:01,509 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2017-12-26 22:11:01,509 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2017-12-26 22:11:01,509 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2017-12-26 22:11:01,513 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 3ms
2017-12-26 22:11:01,513 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 3ms
2017-12-26 22:11:01,515 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 7ms
2017-12-26 22:11:01,658 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-17803564-4596-4115-a494-de0767943c31): no suitable block pools found to scan.  Waiting 1690262497 ms.
2017-12-26 22:11:01,658 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-ceff66d7-ed85-46b1-886b-6a380c086ba2): no suitable block pools found to scan.  Waiting 1690262497 ms.
2017-12-26 22:11:01,728 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 12/26/17 11:33 PM with interval of 21600000ms
2017-12-26 22:11:01,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2017-12-26 22:11:02,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2017-12-26 22:11:02,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-12-26 22:11:02,358 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x9b50b98ec422c09c,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 7 msec to generate and 149 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2017-12-26 22:11:02,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2017-12-26 22:17:55,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741831_1007 src: /192.168.28.131:41762 dest: /192.168.28.132:50010
2017-12-26 22:18:21,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:41762, dest: /192.168.28.132:50010, bytes: 239, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-66565203_29, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741831_1007, duration: 25606792131
2017-12-26 22:18:21,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741831_1007, type=LAST_IN_PIPELINE terminating
2017-12-26 22:18:44,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741832_1008 src: /192.168.28.131:41818 dest: /192.168.28.132:50010
2017-12-26 22:18:44,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:41818, dest: /192.168.28.132:50010, bytes: 1330, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-66565203_29, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741832_1008, duration: 17291648
2017-12-26 22:18:44,275 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741832_1008, type=LAST_IN_PIPELINE terminating
2017-12-26 22:18:44,946 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741833_1009 src: /192.168.28.131:41882 dest: /192.168.28.132:50010
2017-12-26 22:18:44,975 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:41882, dest: /192.168.28.132:50010, bytes: 1337, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-66565203_29, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741833_1009, duration: 27043785
2017-12-26 22:18:44,975 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741833_1009, type=LAST_IN_PIPELINE terminating
2017-12-26 22:18:45,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741834_1010 src: /192.168.28.131:41902 dest: /192.168.28.132:50010
2017-12-26 22:18:45,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:41902, dest: /192.168.28.132:50010, bytes: 1338, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-66565203_29, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741834_1010, duration: 40281522
2017-12-26 22:18:45,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741834_1010, type=LAST_IN_PIPELINE terminating
2017-12-26 22:18:45,521 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741835_1011 src: /192.168.28.131:41926 dest: /192.168.28.132:50010
2017-12-26 22:18:45,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:41926, dest: /192.168.28.132:50010, bytes: 1338, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-66565203_29, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741835_1011, duration: 32238937
2017-12-26 22:18:45,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741835_1011, type=LAST_IN_PIPELINE terminating
2017-12-26 22:18:45,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741836_1012 src: /192.168.28.131:41946 dest: /192.168.28.132:50010
2017-12-26 22:18:45,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:41946, dest: /192.168.28.132:50010, bytes: 1338, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-66565203_29, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741836_1012, duration: 32118387
2017-12-26 22:18:45,855 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741836_1012, type=LAST_IN_PIPELINE terminating
2017-12-26 22:18:46,096 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741837_1013 src: /192.168.28.131:41968 dest: /192.168.28.132:50010
2017-12-26 22:18:46,185 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:41968, dest: /192.168.28.132:50010, bytes: 1338, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-66565203_29, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741837_1013, duration: 86134890
2017-12-26 22:18:46,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741837_1013, type=LAST_IN_PIPELINE terminating
2017-12-26 22:18:49,729 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741838_1014 src: /192.168.28.131:41998 dest: /192.168.28.132:50010
2017-12-26 22:18:51,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:41998, dest: /192.168.28.132:50010, bytes: 260, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-66565203_29, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741838_1014, duration: 2221951978
2017-12-26 22:18:51,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741838_1014, type=LAST_IN_PIPELINE terminating
2017-12-26 22:18:55,408 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741839_1015 src: /192.168.28.131:42004 dest: /192.168.28.132:50010
2017-12-26 22:19:09,687 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:42004, dest: /192.168.28.132:50010, bytes: 260, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-66565203_29, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741839_1015, duration: 14277179961
2017-12-26 22:19:09,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741839_1015, type=LAST_IN_PIPELINE terminating
2017-12-26 22:21:23,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741840_1016 src: /192.168.28.131:42016 dest: /192.168.28.132:50010
2017-12-26 22:21:24,143 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:42016, dest: /192.168.28.132:50010, bytes: 271, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-270104887_29, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741840_1016, duration: 667860403
2017-12-26 22:21:24,143 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741840_1016, type=LAST_IN_PIPELINE terminating
2017-12-26 22:21:25,245 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741841_1017 src: /192.168.28.131:42184 dest: /192.168.28.132:50010
2017-12-26 22:21:25,268 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:42184, dest: /192.168.28.132:50010, bytes: 1362, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-270104887_29, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741841_1017, duration: 19610643
2017-12-26 22:21:25,268 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741841_1017, type=LAST_IN_PIPELINE terminating
2017-12-26 22:21:28,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741842_1018 src: /192.168.28.131:42224 dest: /192.168.28.132:50010
2017-12-26 22:21:29,010 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:42224, dest: /192.168.28.132:50010, bytes: 271, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-270104887_29, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741842_1018, duration: 464223387
2017-12-26 22:21:29,011 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741842_1018, type=LAST_IN_PIPELINE terminating
2017-12-26 22:21:31,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741843_1019 src: /192.168.28.131:42228 dest: /192.168.28.132:50010
2017-12-26 22:22:00,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:42228, dest: /192.168.28.132:50010, bytes: 271, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-270104887_29, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741843_1019, duration: 26238652830
2017-12-26 22:22:00,421 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741843_1019, type=LAST_IN_PIPELINE terminating
2017-12-26 22:44:33,421 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker02.local/192.168.28.132"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2017-12-26 22:44:37,420 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-12-26 22:44:37,963 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2017-12-26 22:44:37,973 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker02.local/192.168.28.132
************************************************************/
2018-01-03 13:16:28,034 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker02.local/192.168.28.132
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-03 13:16:28,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-03 13:16:31,059 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-03 13:16:31,513 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-03 13:16:31,513 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-03 13:16:31,556 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-03 13:16:31,560 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker02.local
2018-01-03 13:16:31,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-03 13:16:31,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-03 13:16:31,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-03 13:16:31,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-03 13:16:32,303 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-03 13:16:32,365 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-03 13:16:32,408 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-03 13:16:32,445 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-03 13:16:32,449 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-03 13:16:32,450 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-03 13:16:32,450 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-03 13:16:32,536 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 35210
2018-01-03 13:16:32,536 INFO org.mortbay.log: jetty-6.1.26
2018-01-03 13:16:33,386 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:35210
2018-01-03 13:16:33,892 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-03 13:16:33,902 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-03 13:16:34,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-03 13:16:34,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-03 13:16:34,901 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-03 13:16:34,952 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-03 13:16:35,279 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-03 13:16:35,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-03 13:16:35,376 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-03 13:16:35,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-03 13:16:35,530 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-03 13:16:35,531 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-03 13:16:36,841 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-03 13:16:36,879 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-03 13:16:36,942 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 2036@hadoop-worker02.local
2018-01-03 13:16:36,982 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 2036@hadoop-worker02.local
2018-01-03 13:16:37,331 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-03 13:16:37,331 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-03 13:16:37,773 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-03 13:16:37,773 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-03 13:16:37,844 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=41fec607-6e2e-4e46-b768-7baf3ef5e9ef
2018-01-03 13:16:38,123 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ceff66d7-ed85-46b1-886b-6a380c086ba2
2018-01-03 13:16:38,123 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-03 13:16:38,138 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-17803564-4596-4115-a494-de0767943c31
2018-01-03 13:16:38,138 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-03 13:16:38,186 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-03 13:16:38,304 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-03 13:16:38,304 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-03 13:16:38,313 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-03 13:16:38,318 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-03 13:16:38,418 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 100ms
2018-01-03 13:16:38,418 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 100ms
2018-01-03 13:16:38,419 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 115ms
2018-01-03 13:16:38,436 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-03 13:16:38,436 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-03 13:16:38,456 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-03 13:16:38,472 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 37ms
2018-01-03 13:16:38,456 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-03 13:16:38,496 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 41ms
2018-01-03 13:16:38,506 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 80ms
2018-01-03 13:16:38,626 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-17803564-4596-4115-a494-de0767943c31): no suitable block pools found to scan.  Waiting 1031125530 ms.
2018-01-03 13:16:38,631 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-ceff66d7-ed85-46b1-886b-6a380c086ba2): no suitable block pools found to scan.  Waiting 1031125524 ms.
2018-01-03 13:16:38,644 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/3/18 4:48 PM with interval of 21600000ms
2018-01-03 13:16:38,691 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-03 13:16:38,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-03 13:16:38,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-03 13:16:39,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x9178e3e4a216762c,  containing 2 storage report(s), of which we sent 2. The reports had 13 total blocks and used 1 RPC(s). This took 10 msec to generate and 312 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-03 13:16:39,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-03 13:41:12,063 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741844_1020 src: /192.168.28.130:55986 dest: /192.168.28.132:50010
2018-01-03 13:41:12,302 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:55986, dest: /192.168.28.132:50010, bytes: 30, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1833690103_1, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741844_1020, duration: 61881831
2018-01-03 13:41:12,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741844_1020, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 13:52:33,427 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741845_1021 src: /192.168.28.132:55588 dest: /192.168.28.132:50010
2018-01-03 13:52:37,439 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:55588, dest: /192.168.28.132:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1069073985_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741845_1021, duration: 3974941544
2018-01-03 13:52:37,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741845_1021, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 13:52:37,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741846_1022 src: /192.168.28.132:55592 dest: /192.168.28.132:50010
2018-01-03 13:52:39,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:55592, dest: /192.168.28.132:50010, bytes: 74090694, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1069073985_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741846_1022, duration: 1921391369
2018-01-03 13:52:39,401 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741846_1022, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 13:52:39,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741847_1023 src: /192.168.28.132:55596 dest: /192.168.28.132:50010
2018-01-03 13:52:39,660 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:55596, dest: /192.168.28.132:50010, bytes: 65912, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1069073985_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741847_1023, duration: 23290948
2018-01-03 13:52:39,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741847_1023, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 13:53:58,242 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741845_1021 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741845 for deletion
2018-01-03 13:53:58,256 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741846_1022 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741846 for deletion
2018-01-03 13:53:58,257 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741847_1023 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741847 for deletion
2018-01-03 13:53:58,261 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741846_1022 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741846
2018-01-03 13:53:58,261 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741845_1021 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741845
2018-01-03 13:53:58,264 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741847_1023 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741847
2018-01-03 13:54:59,217 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741848_1024 src: /192.168.28.132:55786 dest: /192.168.28.132:50010
2018-01-03 13:55:01,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:55786, dest: /192.168.28.132:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1245991119_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741848_1024, duration: 2658244240
2018-01-03 13:55:01,903 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741848_1024, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 13:55:01,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741849_1025 src: /192.168.28.132:55790 dest: /192.168.28.132:50010
2018-01-03 13:55:03,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:55790, dest: /192.168.28.132:50010, bytes: 74090694, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1245991119_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741849_1025, duration: 1597193003
2018-01-03 13:55:03,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741849_1025, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 13:55:03,776 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741850_1026 src: /192.168.28.132:55794 dest: /192.168.28.132:50010
2018-01-03 13:55:03,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:55794, dest: /192.168.28.132:50010, bytes: 65912, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1245991119_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741850_1026, duration: 9001756
2018-01-03 13:55:03,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741850_1026, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 14:00:15,868 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker02.local/192.168.28.132"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-03 14:00:18,386 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-03 14:00:18,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker02.local/192.168.28.132
************************************************************/
2018-01-03 14:03:04,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker02.local/192.168.28.132
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-03 14:03:04,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-03 14:03:06,434 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-03 14:03:06,980 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-03 14:03:06,980 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-03 14:03:07,006 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-03 14:03:07,021 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker02.local
2018-01-03 14:03:07,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-03 14:03:07,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-03 14:03:07,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-03 14:03:07,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-03 14:03:08,031 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-03 14:03:08,171 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-03 14:03:08,191 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-03 14:03:08,202 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-03 14:03:08,204 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-03 14:03:08,205 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-03 14:03:08,205 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-03 14:03:08,259 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 34849
2018-01-03 14:03:08,260 INFO org.mortbay.log: jetty-6.1.26
2018-01-03 14:03:08,972 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:34849
2018-01-03 14:03:09,866 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-03 14:03:09,975 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-03 14:03:10,667 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-03 14:03:10,667 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-03 14:03:10,895 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-03 14:03:10,936 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-03 14:03:11,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-03 14:03:11,213 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-03 14:03:11,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-03 14:03:11,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-03 14:03:11,514 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-03 14:03:11,512 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-03 14:03:12,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-03 14:03:12,727 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-03 14:03:12,996 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 23947@hadoop-worker02.local
2018-01-03 14:03:13,011 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 23947@hadoop-worker02.local
2018-01-03 14:03:13,364 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-03 14:03:13,364 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-03 14:03:13,943 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-03 14:03:13,943 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-03 14:03:13,958 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=41fec607-6e2e-4e46-b768-7baf3ef5e9ef
2018-01-03 14:03:14,293 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ceff66d7-ed85-46b1-886b-6a380c086ba2
2018-01-03 14:03:14,294 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-03 14:03:14,310 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-17803564-4596-4115-a494-de0767943c31
2018-01-03 14:03:14,310 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-03 14:03:14,419 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-03 14:03:14,551 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-03 14:03:14,552 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-03 14:03:14,575 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-03 14:03:14,577 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-03 14:03:14,755 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current: 135426048
2018-01-03 14:03:14,763 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current: 74739712
2018-01-03 14:03:14,768 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 159ms
2018-01-03 14:03:14,821 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 212ms
2018-01-03 14:03:14,838 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 286ms
2018-01-03 14:03:14,940 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-03 14:03:14,941 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-03 14:03:14,983 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 42ms
2018-01-03 14:03:14,988 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-03 14:03:14,988 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-03 14:03:15,073 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 86ms
2018-01-03 14:03:15,079 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 157ms
2018-01-03 14:03:15,347 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-17803564-4596-4115-a494-de0767943c31): no suitable block pools found to scan.  Waiting 1028328809 ms.
2018-01-03 14:03:15,364 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/3/18 2:59 PM with interval of 21600000ms
2018-01-03 14:03:15,372 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-ceff66d7-ed85-46b1-886b-6a380c086ba2): no suitable block pools found to scan.  Waiting 1028328783 ms.
2018-01-03 14:03:15,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-03 14:03:15,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-03 14:03:15,861 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-03 14:03:16,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x485ea9c4f7ce1ca3,  containing 2 storage report(s), of which we sent 2. The reports had 17 total blocks and used 1 RPC(s). This took 17 msec to generate and 174 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-03 14:03:16,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-03 14:04:29,195 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741851_1027 src: /192.168.28.132:56070 dest: /192.168.28.132:50010
2018-01-03 14:04:34,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:56070, dest: /192.168.28.132:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1767299747_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741851_1027, duration: 4664613193
2018-01-03 14:04:34,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741851_1027, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 14:04:34,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741852_1028 src: /192.168.28.132:56074 dest: /192.168.28.132:50010
2018-01-03 14:04:35,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:56074, dest: /192.168.28.132:50010, bytes: 74090694, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1767299747_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741852_1028, duration: 1820580500
2018-01-03 14:04:35,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741852_1028, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 14:04:36,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741853_1029 src: /192.168.28.132:56078 dest: /192.168.28.132:50010
2018-01-03 14:04:36,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:56078, dest: /192.168.28.132:50010, bytes: 66101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1767299747_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741853_1029, duration: 20803517
2018-01-03 14:04:36,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741853_1029, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 14:08:05,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741851_1027 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741851 for deletion
2018-01-03 14:08:05,865 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741852_1028 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741852 for deletion
2018-01-03 14:08:05,866 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741853_1029 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741853 for deletion
2018-01-03 14:08:05,867 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741852_1028 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741852
2018-01-03 14:08:05,869 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741851_1027 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741851
2018-01-03 14:08:05,870 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741853_1029 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741853
2018-01-03 14:08:26,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741854_1030 src: /192.168.28.132:56260 dest: /192.168.28.132:50010
2018-01-03 14:08:29,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:56260, dest: /192.168.28.132:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1449670756_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741854_1030, duration: 2883110755
2018-01-03 14:08:29,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741854_1030, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 14:08:29,275 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741855_1031 src: /192.168.28.132:56264 dest: /192.168.28.132:50010
2018-01-03 14:08:30,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:56264, dest: /192.168.28.132:50010, bytes: 74090694, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1449670756_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741855_1031, duration: 1524347784
2018-01-03 14:08:30,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741855_1031, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 14:08:31,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741856_1032 src: /192.168.28.132:56268 dest: /192.168.28.132:50010
2018-01-03 14:08:31,131 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:56268, dest: /192.168.28.132:50010, bytes: 66101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1449670756_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741856_1032, duration: 8426478
2018-01-03 14:08:31,131 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741856_1032, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 14:09:44,765 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741856_1032 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741856 for deletion
2018-01-03 14:09:44,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741854_1030 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741854 for deletion
2018-01-03 14:09:44,797 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741855_1031 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741855 for deletion
2018-01-03 14:09:44,803 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741856_1032 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741856
2018-01-03 14:09:44,806 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741855_1031 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741855
2018-01-03 14:09:44,807 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741854_1030 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741854
2018-01-03 14:10:12,670 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741857_1033 src: /192.168.28.132:56466 dest: /192.168.28.132:50010
2018-01-03 14:10:15,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:56466, dest: /192.168.28.132:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-703799157_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741857_1033, duration: 3002704425
2018-01-03 14:10:15,704 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741857_1033, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 14:10:15,732 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741858_1034 src: /192.168.28.132:56470 dest: /192.168.28.132:50010
2018-01-03 14:10:17,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:56470, dest: /192.168.28.132:50010, bytes: 74090694, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-703799157_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741858_1034, duration: 1468745679
2018-01-03 14:10:17,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741858_1034, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 14:10:17,529 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741859_1035 src: /192.168.28.132:56474 dest: /192.168.28.132:50010
2018-01-03 14:10:17,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:56474, dest: /192.168.28.132:50010, bytes: 66101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-703799157_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741859_1035, duration: 10466428
2018-01-03 14:10:17,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741859_1035, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 14:19:45,320 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741857_1033 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741857 for deletion
2018-01-03 14:19:45,347 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741858_1034 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741858 for deletion
2018-01-03 14:19:45,349 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741859_1035 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741859 for deletion
2018-01-03 14:19:45,352 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741857_1033 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741857
2018-01-03 14:19:45,353 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741858_1034 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741858
2018-01-03 14:19:45,353 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741859_1035 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741859
2018-01-03 14:20:14,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741860_1036 src: /192.168.28.132:56754 dest: /192.168.28.132:50010
2018-01-03 14:20:17,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:56754, dest: /192.168.28.132:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_777984038_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741860_1036, duration: 3422738841
2018-01-03 14:20:17,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741860_1036, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 14:20:17,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741861_1037 src: /192.168.28.132:56758 dest: /192.168.28.132:50010
2018-01-03 14:20:18,919 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:56758, dest: /192.168.28.132:50010, bytes: 74090694, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_777984038_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741861_1037, duration: 1317029757
2018-01-03 14:20:18,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741861_1037, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 14:20:19,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741862_1038 src: /192.168.28.132:56762 dest: /192.168.28.132:50010
2018-01-03 14:20:19,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:56762, dest: /192.168.28.132:50010, bytes: 66101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_777984038_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741862_1038, duration: 8759184
2018-01-03 14:20:19,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741862_1038, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 14:26:45,119 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741860_1036 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741860 for deletion
2018-01-03 14:26:45,121 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741861_1037 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741861 for deletion
2018-01-03 14:26:45,123 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741862_1038 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741862 for deletion
2018-01-03 14:26:45,145 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741860_1036 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741860
2018-01-03 14:26:45,148 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741862_1038 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741862
2018-01-03 14:26:45,187 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741861_1037 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741861
2018-01-03 14:29:18,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741863_1039 src: /192.168.28.132:56808 dest: /192.168.28.132:50010
2018-01-03 14:29:21,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:56808, dest: /192.168.28.132:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1274515486_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741863_1039, duration: 2954835926
2018-01-03 14:29:21,421 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741863_1039, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 14:29:21,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741864_1040 src: /192.168.28.132:56812 dest: /192.168.28.132:50010
2018-01-03 14:29:23,027 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:56812, dest: /192.168.28.132:50010, bytes: 74090694, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1274515486_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741864_1040, duration: 1536982917
2018-01-03 14:29:23,027 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741864_1040, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 14:29:23,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741865_1041 src: /192.168.28.132:56816 dest: /192.168.28.132:50010
2018-01-03 14:29:23,386 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:56816, dest: /192.168.28.132:50010, bytes: 66101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1274515486_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741865_1041, duration: 35218430
2018-01-03 14:29:23,386 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741865_1041, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 14:35:29,689 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741863_1039 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741863 for deletion
2018-01-03 14:35:29,691 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741864_1040 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741864 for deletion
2018-01-03 14:35:29,691 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741865_1041 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741865 for deletion
2018-01-03 14:35:29,693 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741864_1040 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741864
2018-01-03 14:35:29,694 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741863_1039 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741863
2018-01-03 14:35:29,694 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741865_1041 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741865
2018-01-03 15:01:44,385 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-199680249-192.168.28.129-1514169681712 Total blocks: 17, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-01-03 15:18:30,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741866_1042 src: /192.168.28.130:56884 dest: /192.168.28.132:50010
2018-01-03 15:18:36,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:56884, dest: /192.168.28.132:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1373847299_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741866_1042, duration: 5427405011
2018-01-03 15:18:36,278 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741866_1042, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 15:18:36,305 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741867_1043 src: /192.168.28.130:56886 dest: /192.168.28.132:50010
2018-01-03 15:18:38,672 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:56886, dest: /192.168.28.132:50010, bytes: 74090694, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1373847299_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741867_1043, duration: 2348296713
2018-01-03 15:18:38,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741867_1043, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 15:18:39,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741868_1044 src: /192.168.28.131:59268 dest: /192.168.28.132:50010
2018-01-03 15:18:39,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:59268, dest: /192.168.28.132:50010, bytes: 66101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1373847299_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741868_1044, duration: 87302177
2018-01-03 15:18:39,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741868_1044, type=LAST_IN_PIPELINE terminating
2018-01-03 15:24:50,964 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741866_1042 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741866 for deletion
2018-01-03 15:24:50,966 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741867_1043 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741867 for deletion
2018-01-03 15:24:50,968 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741868_1044 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741868 for deletion
2018-01-03 15:24:50,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741866_1042 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741866
2018-01-03 15:24:50,988 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741868_1044 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741868
2018-01-03 15:24:50,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741867_1043 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741867
2018-01-03 15:27:32,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741869_1045 src: /192.168.28.131:59270 dest: /192.168.28.132:50010
2018-01-03 15:27:37,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:59270, dest: /192.168.28.132:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-85016196_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741869_1045, duration: 4624502614
2018-01-03 15:27:37,450 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741869_1045, type=LAST_IN_PIPELINE terminating
2018-01-03 15:27:37,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741870_1046 src: /192.168.28.131:59272 dest: /192.168.28.132:50010
2018-01-03 15:27:40,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:59272, dest: /192.168.28.132:50010, bytes: 74090694, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-85016196_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741870_1046, duration: 2589867920
2018-01-03 15:27:40,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741870_1046, type=LAST_IN_PIPELINE terminating
2018-01-03 15:27:40,857 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741871_1047 src: /192.168.28.130:57110 dest: /192.168.28.132:50010
2018-01-03 15:27:40,876 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:57110, dest: /192.168.28.132:50010, bytes: 66101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-85016196_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741871_1047, duration: 10786333
2018-01-03 15:27:40,877 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741871_1047, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 15:34:20,512 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741869_1045 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741869 for deletion
2018-01-03 15:34:20,516 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741870_1046 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741870 for deletion
2018-01-03 15:34:20,518 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741871_1047 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741871 for deletion
2018-01-03 15:34:20,542 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741869_1045 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741869
2018-01-03 15:34:20,542 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741871_1047 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741871
2018-01-03 15:34:20,550 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741870_1046 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741870
2018-01-03 15:36:07,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741872_1048 src: /192.168.28.131:59274 dest: /192.168.28.132:50010
2018-01-03 15:36:11,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:59274, dest: /192.168.28.132:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-938491162_1, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741872_1048, duration: 3858035053
2018-01-03 15:36:11,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741872_1048, type=LAST_IN_PIPELINE terminating
2018-01-03 15:36:11,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741873_1049 src: /192.168.28.131:59276 dest: /192.168.28.132:50010
2018-01-03 15:36:13,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:59276, dest: /192.168.28.132:50010, bytes: 74090689, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-938491162_1, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741873_1049, duration: 2280237298
2018-01-03 15:36:13,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741873_1049, type=LAST_IN_PIPELINE terminating
2018-01-03 15:36:14,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741874_1050 src: /192.168.28.131:59278 dest: /192.168.28.132:50010
2018-01-03 15:36:14,146 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:59278, dest: /192.168.28.132:50010, bytes: 66174, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-938491162_1, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741874_1050, duration: 6054869
2018-01-03 15:36:14,146 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741874_1050, type=LAST_IN_PIPELINE terminating
2018-01-03 15:38:35,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741875_1051 src: /192.168.28.131:59280 dest: /192.168.28.132:50010
2018-01-03 15:38:39,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:59280, dest: /192.168.28.132:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1509510445_1, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741875_1051, duration: 3966112100
2018-01-03 15:38:39,131 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741875_1051, type=LAST_IN_PIPELINE terminating
2018-01-03 15:38:39,241 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741876_1052 src: /192.168.28.130:57166 dest: /192.168.28.132:50010
2018-01-03 15:38:41,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:57166, dest: /192.168.28.132:50010, bytes: 74090689, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1509510445_1, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741876_1052, duration: 2176020821
2018-01-03 15:38:41,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741876_1052, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 15:38:41,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741877_1053 src: /192.168.28.130:57168 dest: /192.168.28.132:50010
2018-01-03 15:38:41,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:57168, dest: /192.168.28.132:50010, bytes: 66173, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1509510445_1, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741877_1053, duration: 42951793
2018-01-03 15:38:41,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741877_1053, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 15:41:08,087 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741872_1048 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741872 for deletion
2018-01-03 15:41:08,089 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741873_1049 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741873 for deletion
2018-01-03 15:41:08,090 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741874_1050 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741874 for deletion
2018-01-03 15:41:08,092 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741873_1049 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741873
2018-01-03 15:41:08,094 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741872_1048 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741872
2018-01-03 15:41:08,094 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741874_1050 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741874
2018-01-03 15:44:16,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741875_1051 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741875 for deletion
2018-01-03 15:44:16,472 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741876_1052 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741876 for deletion
2018-01-03 15:44:16,510 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741877_1053 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741877 for deletion
2018-01-03 15:44:16,512 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741875_1051 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741875
2018-01-03 15:44:16,513 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741877_1053 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741877
2018-01-03 15:44:16,520 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741876_1052 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741876
2018-01-03 15:45:40,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741878_1054 src: /192.168.28.130:57188 dest: /192.168.28.132:50010
2018-01-03 15:45:43,736 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:57188, dest: /192.168.28.132:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1910030958_1, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741878_1054, duration: 3176655757
2018-01-03 15:45:43,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741878_1054, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 15:45:43,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741879_1055 src: /192.168.28.131:60994 dest: /192.168.28.132:50010
2018-01-03 15:45:45,282 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:60994, dest: /192.168.28.132:50010, bytes: 75090816, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1910030958_1, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741879_1055, duration: 1528258386
2018-01-03 15:45:45,282 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741879_1055, type=LAST_IN_PIPELINE terminating
2018-01-03 15:45:45,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741880_1056 src: /192.168.28.130:57194 dest: /192.168.28.132:50010
2018-01-03 15:45:45,592 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:57194, dest: /192.168.28.132:50010, bytes: 66174, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1910030958_1, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741880_1056, duration: 7693011
2018-01-03 15:45:45,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741880_1056, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-03 15:47:02,551 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741878_1054 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741878 for deletion
2018-01-03 15:47:02,553 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741879_1055 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741879 for deletion
2018-01-03 15:47:02,554 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741880_1056 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741880 for deletion
2018-01-03 15:47:02,555 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741878_1054 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741878
2018-01-03 15:47:02,555 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741880_1056 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741880
2018-01-03 15:47:02,555 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741879_1055 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741879
2018-01-03 15:48:07,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741881_1057 src: /192.168.28.131:32788 dest: /192.168.28.132:50010
2018-01-03 15:48:10,314 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:32788, dest: /192.168.28.132:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-108787594_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741881_1057, duration: 3276211578
2018-01-03 15:48:10,315 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741881_1057, type=LAST_IN_PIPELINE terminating
2018-01-03 15:48:10,351 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741882_1058 src: /192.168.28.131:32790 dest: /192.168.28.132:50010
2018-01-03 15:48:12,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:32790, dest: /192.168.28.132:50010, bytes: 75090821, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-108787594_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741882_1058, duration: 1865644348
2018-01-03 15:48:12,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741882_1058, type=LAST_IN_PIPELINE terminating
2018-01-03 15:48:12,476 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741883_1059 src: /192.168.28.131:32792 dest: /192.168.28.132:50010
2018-01-03 15:48:12,487 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:32792, dest: /192.168.28.132:50010, bytes: 66101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-108787594_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741883_1059, duration: 9396605
2018-01-03 15:48:12,487 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741883_1059, type=LAST_IN_PIPELINE terminating
2018-01-03 15:51:27,745 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker02.local/192.168.28.132"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-03 15:51:31,146 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-03 15:51:31,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker02.local/192.168.28.132
************************************************************/
2018-01-05 14:57:18,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker02.local/192.168.28.132
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-05 14:57:18,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-05 14:57:21,465 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-05 14:57:21,977 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-05 14:57:21,977 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-05 14:57:21,993 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-05 14:57:21,996 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker02.local
2018-01-05 14:57:22,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-05 14:57:22,084 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-05 14:57:22,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-05 14:57:22,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-05 14:57:22,761 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-05 14:57:22,825 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-05 14:57:23,070 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-05 14:57:23,079 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-05 14:57:23,082 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-05 14:57:23,082 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-05 14:57:23,082 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-05 14:57:23,123 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 42178
2018-01-05 14:57:23,123 INFO org.mortbay.log: jetty-6.1.26
2018-01-05 14:57:24,055 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:42178
2018-01-05 14:57:24,610 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-05 14:57:24,671 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-05 14:57:25,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-05 14:57:25,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-05 14:57:25,833 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-05 14:57:25,899 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-05 14:57:26,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-05 14:57:26,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-05 14:57:26,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-05 14:57:26,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-05 14:57:26,467 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-05 14:57:26,452 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-05 14:57:30,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-05 14:57:30,674 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-05 14:57:30,725 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 1830@hadoop-worker02.local
2018-01-05 14:57:30,749 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 1830@hadoop-worker02.local
2018-01-05 14:57:31,365 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-05 14:57:31,366 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-05 14:57:31,645 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-05 14:57:31,645 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-05 14:57:31,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=41fec607-6e2e-4e46-b768-7baf3ef5e9ef
2018-01-05 14:57:32,095 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ceff66d7-ed85-46b1-886b-6a380c086ba2
2018-01-05 14:57:32,095 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-05 14:57:32,111 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-17803564-4596-4115-a494-de0767943c31
2018-01-05 14:57:32,111 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-05 14:57:32,134 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-05 14:57:32,193 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-05 14:57:32,194 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-05 14:57:32,229 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-05 14:57:32,343 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-05 14:57:33,038 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 725ms
2018-01-05 14:57:33,187 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 843ms
2018-01-05 14:57:33,197 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 990ms
2018-01-05 14:57:33,203 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-05 14:57:33,204 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-05 14:57:33,206 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-05 14:57:33,206 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-05 14:57:33,230 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 27ms
2018-01-05 14:57:33,245 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 39ms
2018-01-05 14:57:33,246 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 44ms
2018-01-05 14:57:33,387 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-17803564-4596-4115-a494-de0767943c31): no suitable block pools found to scan.  Waiting 852270768 ms.
2018-01-05 14:57:33,406 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-ceff66d7-ed85-46b1-886b-6a380c086ba2): no suitable block pools found to scan.  Waiting 852270749 ms.
2018-01-05 14:57:33,623 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/5/18 4:20 PM with interval of 21600000ms
2018-01-05 14:57:33,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-05 14:57:33,734 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-05 14:57:33,734 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-05 14:57:34,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1f11b44c3ab7f2fb,  containing 2 storage report(s), of which we sent 2. The reports had 20 total blocks and used 1 RPC(s). This took 17 msec to generate and 334 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-05 14:57:34,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-05 15:28:01,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1f11b44c3ab7f2fc,  containing 2 storage report(s), of which we sent 2. The reports had 20 total blocks and used 1 RPC(s). This took 3 msec to generate and 22 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-05 15:28:01,607 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-05 15:37:02,794 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741884_1060 src: /192.168.28.131:52538 dest: /192.168.28.132:50010
2018-01-05 15:37:02,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:52538, dest: /192.168.28.132:50010, bytes: 63213, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_989782417_1, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741884_1060, duration: 125962169
2018-01-05 15:37:02,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741884_1060, type=LAST_IN_PIPELINE terminating
2018-01-05 15:38:54,014 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741885_1061 src: /192.168.28.131:52540 dest: /192.168.28.132:50010
2018-01-05 15:39:01,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:52540, dest: /192.168.28.132:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1259734289_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741885_1061, duration: 7045900731
2018-01-05 15:39:01,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741885_1061, type=LAST_IN_PIPELINE terminating
2018-01-05 15:39:01,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741886_1062 src: /192.168.28.130:33756 dest: /192.168.28.132:50010
2018-01-05 15:39:05,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:33756, dest: /192.168.28.132:50010, bytes: 75090821, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1259734289_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741886_1062, duration: 4617508471
2018-01-05 15:39:05,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741886_1062, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-05 15:39:06,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741887_1063 src: /192.168.28.130:33758 dest: /192.168.28.132:50010
2018-01-05 15:39:06,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:33758, dest: /192.168.28.132:50010, bytes: 66101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1259734289_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741887_1063, duration: 11822345
2018-01-05 15:39:06,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741887_1063, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-05 15:39:49,733 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2099ms
No GCs detected
2018-01-05 15:39:58,318 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2134ms
No GCs detected
2018-01-05 15:40:04,095 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4746ms
No GCs detected
2018-01-05 15:40:13,368 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4282ms
No GCs detected
2018-01-05 15:40:22,010 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1342ms
No GCs detected
2018-01-05 15:40:24,360 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1849ms
No GCs detected
2018-01-05 15:40:34,807 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 9189ms
No GCs detected
2018-01-05 15:40:50,641 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 11570ms
No GCs detected
2018-01-05 15:40:56,733 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 5592ms
No GCs detected
2018-01-05 15:41:01,906 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1415ms
No GCs detected
2018-01-05 16:59:18,654 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1276ms
No GCs detected
2018-01-05 16:59:21,059 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1905ms
No GCs detected
2018-01-05 16:59:22,731 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1171ms
No GCs detected
2018-01-05 16:59:30,893 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1725ms
No GCs detected
2018-01-05 17:02:26,108 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741885_1061 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741885 for deletion
2018-01-05 17:02:26,120 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741886_1062 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741886 for deletion
2018-01-05 17:02:26,121 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741887_1063 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741887 for deletion
2018-01-05 17:02:26,123 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741886_1062 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741886
2018-01-05 17:02:26,124 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741885_1061 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741885
2018-01-05 17:02:26,125 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741887_1063 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741887
2018-01-05 17:02:50,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741888_1064 src: /192.168.28.130:34032 dest: /192.168.28.132:50010
2018-01-05 17:02:55,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:34032, dest: /192.168.28.132:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-502258946_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741888_1064, duration: 5334286020
2018-01-05 17:02:55,915 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741888_1064, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-05 17:02:56,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741889_1065 src: /192.168.28.130:34034 dest: /192.168.28.132:50010
2018-01-05 17:02:58,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:34034, dest: /192.168.28.132:50010, bytes: 75090821, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-502258946_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741889_1065, duration: 2895738041
2018-01-05 17:02:58,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741889_1065, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-05 17:02:59,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741890_1066 src: /192.168.28.131:52584 dest: /192.168.28.132:50010
2018-01-05 17:02:59,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:52584, dest: /192.168.28.132:50010, bytes: 66101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-502258946_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741890_1066, duration: 8544859
2018-01-05 17:02:59,626 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741890_1066, type=LAST_IN_PIPELINE terminating
2018-01-05 17:03:03,532 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1568ms
No GCs detected
2018-01-05 17:04:09,552 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2386ms
No GCs detected
2018-01-05 17:06:50,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741891_1067 src: /192.168.28.131:52616 dest: /192.168.28.132:50010
2018-01-05 17:06:50,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:52616, dest: /192.168.28.132:50010, bytes: 63213, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_253152837_1, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741891_1067, duration: 34352509
2018-01-05 17:06:50,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741891_1067, type=LAST_IN_PIPELINE terminating
2018-01-05 17:06:52,013 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741884_1060 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741884 for deletion
2018-01-05 17:06:52,029 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741884_1060 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741884
2018-01-05 17:15:18,502 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1839ms
No GCs detected
2018-01-05 17:34:23,186 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741888_1064 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741888 for deletion
2018-01-05 17:34:23,192 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741889_1065 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741889 for deletion
2018-01-05 17:34:23,195 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741890_1066 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741890 for deletion
2018-01-05 17:34:23,202 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741888_1064 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741888
2018-01-05 17:34:23,203 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741890_1066 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741890
2018-01-05 17:34:23,213 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741889_1065 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741889
2018-01-05 17:35:15,768 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741892_1068 src: /192.168.28.131:52626 dest: /192.168.28.132:50010
2018-01-05 17:35:20,993 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:52626, dest: /192.168.28.132:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973549_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741892_1068, duration: 5187900920
2018-01-05 17:35:21,006 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741892_1068, type=LAST_IN_PIPELINE terminating
2018-01-05 17:35:21,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741893_1069 src: /192.168.28.130:34402 dest: /192.168.28.132:50010
2018-01-05 17:35:23,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:34402, dest: /192.168.28.132:50010, bytes: 75090821, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973549_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741893_1069, duration: 2638721395
2018-01-05 17:35:23,875 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741893_1069, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-05 17:35:24,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741894_1070 src: /192.168.28.131:52628 dest: /192.168.28.132:50010
2018-01-05 17:35:24,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:52628, dest: /192.168.28.132:50010, bytes: 66101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973549_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741894_1070, duration: 163704215
2018-01-05 17:35:24,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741894_1070, type=LAST_IN_PIPELINE terminating
2018-01-05 17:37:21,701 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2767ms
No GCs detected
2018-01-05 17:37:47,131 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 12500ms
No GCs detected
2018-01-05 17:37:52,475 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3408ms
No GCs detected
2018-01-05 17:39:07,306 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 20335ms
No GCs detected
2018-01-05 17:40:18,361 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-199680249-192.168.28.129-1514169681712 Total blocks: 24, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-01-05 17:41:10,579 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741892_1068 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741892 for deletion
2018-01-05 17:41:10,580 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741893_1069 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741893 for deletion
2018-01-05 17:41:10,585 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741892_1068 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741892
2018-01-05 17:41:10,587 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741894_1070 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741894 for deletion
2018-01-05 17:41:10,589 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741894_1070 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741894
2018-01-05 17:41:10,591 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741893_1069 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741893
2018-01-05 19:01:36,238 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: "hadoop-worker02.local/192.168.28.132"; destination host is: "hadoop-master":8020; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:554)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-05 19:01:39,918 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-05 19:01:40,921 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-05 19:01:41,837 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-05 19:01:41,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker02.local/192.168.28.132
************************************************************/
2018-01-07 12:44:25,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker02.local/192.168.28.132
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-07 12:44:25,278 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-07 12:44:28,408 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-07 12:44:28,736 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-07 12:44:28,736 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-07 12:44:28,755 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-07 12:44:28,757 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker02.local
2018-01-07 12:44:28,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-07 12:44:28,849 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-07 12:44:28,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-07 12:44:28,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-07 12:44:29,401 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-07 12:44:29,458 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-07 12:44:29,496 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-07 12:44:29,506 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-07 12:44:29,509 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-07 12:44:29,509 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-07 12:44:29,509 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-07 12:44:29,561 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 38288
2018-01-07 12:44:29,561 INFO org.mortbay.log: jetty-6.1.26
2018-01-07 12:44:30,756 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:38288
2018-01-07 12:44:31,913 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-07 12:44:31,922 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-07 12:44:33,147 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-07 12:44:33,147 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-07 12:44:33,590 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-07 12:44:33,690 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-07 12:44:34,517 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-07 12:44:34,542 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-07 12:44:34,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-07 12:44:34,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-07 12:44:35,136 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-07 12:44:35,137 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-07 12:44:39,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-07 12:44:39,217 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-07 12:44:39,321 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 1733@hadoop-worker02.local
2018-01-07 12:44:39,782 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 1733@hadoop-worker02.local
2018-01-07 12:44:40,120 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-07 12:44:40,120 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-07 12:44:41,521 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-07 12:44:41,521 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-07 12:44:41,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=41fec607-6e2e-4e46-b768-7baf3ef5e9ef
2018-01-07 12:44:41,714 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ceff66d7-ed85-46b1-886b-6a380c086ba2
2018-01-07 12:44:41,714 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-07 12:44:41,736 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-17803564-4596-4115-a494-de0767943c31
2018-01-07 12:44:41,736 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-07 12:44:41,875 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-07 12:44:41,906 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-07 12:44:41,906 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-07 12:44:41,911 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-07 12:44:41,946 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-07 12:44:42,326 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 380ms
2018-01-07 12:44:42,342 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 395ms
2018-01-07 12:44:42,349 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 443ms
2018-01-07 12:44:42,365 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-07 12:44:42,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-07 12:44:42,376 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-07 12:44:42,386 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-07 12:44:42,450 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 74ms
2018-01-07 12:44:42,451 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 85ms
2018-01-07 12:44:42,470 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 118ms
2018-01-07 12:44:42,687 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-17803564-4596-4115-a494-de0767943c31): no suitable block pools found to scan.  Waiting 687441468 ms.
2018-01-07 12:44:42,696 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-ceff66d7-ed85-46b1-886b-6a380c086ba2): no suitable block pools found to scan.  Waiting 687441459 ms.
2018-01-07 12:44:42,809 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/7/18 2:07 PM with interval of 21600000ms
2018-01-07 12:44:42,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-07 12:44:43,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-07 12:44:43,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-07 12:44:52,141 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xa299a0af07171dc9,  containing 2 storage report(s), of which we sent 2. The reports had 21 total blocks and used 1 RPC(s). This took 31 msec to generate and 141 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-07 12:44:52,141 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-07 13:07:46,419 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker02.local/192.168.28.132"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-07 13:07:50,190 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-07 13:07:50,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker02.local/192.168.28.132
************************************************************/
2018-01-07 13:08:34,279 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker02.local/192.168.28.132
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-07 13:08:34,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-07 13:08:36,423 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-07 13:08:36,615 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-07 13:08:36,615 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-07 13:08:36,628 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-07 13:08:36,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker02.local
2018-01-07 13:08:36,648 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-07 13:08:36,769 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-07 13:08:36,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-07 13:08:36,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-07 13:08:37,438 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-07 13:08:37,513 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-07 13:08:37,536 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-07 13:08:37,549 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-07 13:08:37,552 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-07 13:08:37,552 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-07 13:08:37,552 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-07 13:08:37,615 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 33886
2018-01-07 13:08:37,615 INFO org.mortbay.log: jetty-6.1.26
2018-01-07 13:08:38,406 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:33886
2018-01-07 13:08:39,407 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-07 13:08:39,429 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-07 13:08:40,414 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-07 13:08:40,414 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-07 13:08:40,776 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-07 13:08:40,815 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-07 13:08:41,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-07 13:08:41,315 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-07 13:08:41,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-07 13:08:41,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-07 13:08:41,623 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-07 13:08:41,625 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-07 13:08:42,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-07 13:08:42,178 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-07 13:08:42,288 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 2447@hadoop-worker02.local
2018-01-07 13:08:42,511 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 2447@hadoop-worker02.local
2018-01-07 13:08:42,620 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-07 13:08:42,620 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-07 13:08:43,215 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-07 13:08:43,215 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-07 13:08:43,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=41fec607-6e2e-4e46-b768-7baf3ef5e9ef
2018-01-07 13:08:43,439 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ceff66d7-ed85-46b1-886b-6a380c086ba2
2018-01-07 13:08:43,439 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-07 13:08:43,440 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-17803564-4596-4115-a494-de0767943c31
2018-01-07 13:08:43,440 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-07 13:08:43,497 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-07 13:08:43,516 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-07 13:08:43,516 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-07 13:08:43,520 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-07 13:08:43,520 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-07 13:08:43,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current: 150491136
2018-01-07 13:08:43,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current: 270770176
2018-01-07 13:08:43,574 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 53ms
2018-01-07 13:08:43,575 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 54ms
2018-01-07 13:08:43,575 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 59ms
2018-01-07 13:08:43,580 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-07 13:08:43,580 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-07 13:08:43,580 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-07 13:08:43,580 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-07 13:08:43,589 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 9ms
2018-01-07 13:08:43,590 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 10ms
2018-01-07 13:08:43,591 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 12ms
2018-01-07 13:08:43,762 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/7/18 2:34 PM with interval of 21600000ms
2018-01-07 13:08:43,763 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-ceff66d7-ed85-46b1-886b-6a380c086ba2): no suitable block pools found to scan.  Waiting 686000392 ms.
2018-01-07 13:08:43,778 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-17803564-4596-4115-a494-de0767943c31): no suitable block pools found to scan.  Waiting 686000377 ms.
2018-01-07 13:08:43,969 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-07 13:08:44,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-07 13:08:44,345 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-07 13:08:45,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x65ace2c35cd7ea1e,  containing 2 storage report(s), of which we sent 2. The reports had 21 total blocks and used 1 RPC(s). This took 10 msec to generate and 144 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-07 13:08:45,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-07 13:11:01,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741895_1071 src: /192.168.28.131:46352 dest: /192.168.28.132:50010
2018-01-07 13:11:01,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:46352, dest: /192.168.28.132:50010, bytes: 399, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-347199629_45, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741895_1071, duration: 118230969
2018-01-07 13:11:01,302 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741895_1071, type=LAST_IN_PIPELINE terminating
2018-01-07 13:13:31,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741896_1072 src: /192.168.28.131:46354 dest: /192.168.28.132:50010
2018-01-07 13:13:40,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:46354, dest: /192.168.28.132:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1585732224_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741896_1072, duration: 9041319801
2018-01-07 13:13:40,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741896_1072, type=LAST_IN_PIPELINE terminating
2018-01-07 13:13:40,423 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741897_1073 src: /192.168.28.130:33624 dest: /192.168.28.132:50010
2018-01-07 13:13:45,875 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:33624, dest: /192.168.28.132:50010, bytes: 75090821, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1585732224_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741897_1073, duration: 5354499174
2018-01-07 13:13:45,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741897_1073, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-07 13:13:46,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741898_1074 src: /192.168.28.130:33626 dest: /192.168.28.132:50010
2018-01-07 13:13:46,804 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:33626, dest: /192.168.28.132:50010, bytes: 66091, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1585732224_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741898_1074, duration: 18150753
2018-01-07 13:13:46,805 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741898_1074, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-07 13:14:24,942 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1913ms
No GCs detected
2018-01-07 13:14:30,492 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2907ms
No GCs detected
2018-01-07 13:14:40,329 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2494ms
No GCs detected
2018-01-07 13:14:48,195 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1382ms
No GCs detected
2018-01-07 13:14:52,828 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4133ms
No GCs detected
2018-01-07 13:17:34,096 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 142496ms
No GCs detected
2018-01-07 13:17:49,756 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 10418ms
No GCs detected
2018-01-07 13:17:57,677 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 5857ms
No GCs detected
2018-01-07 13:18:03,176 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4998ms
No GCs detected
2018-01-07 13:18:05,621 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1944ms
No GCs detected
2018-01-07 13:18:15,645 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 5654ms
No GCs detected
2018-01-07 13:18:26,463 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3270ms
No GCs detected
2018-01-07 13:18:35,727 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4287ms
No GCs detected
2018-01-07 13:18:46,685 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1891ms
No GCs detected
2018-01-07 13:18:55,165 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4030ms
No GCs detected
2018-01-07 13:19:18,503 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 19531ms
No GCs detected
2018-01-07 13:19:56,009 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 37005ms
No GCs detected
2018-01-07 13:20:09,258 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 12749ms
No GCs detected
2018-01-07 13:20:16,399 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 6641ms
No GCs detected
2018-01-07 13:20:20,455 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3556ms
No GCs detected
2018-01-07 13:20:22,655 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1699ms
No GCs detected
2018-01-07 13:20:29,091 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 5936ms
No GCs detected
2018-01-07 13:20:33,558 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3966ms
No GCs detected
2018-01-07 13:20:37,087 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3028ms
No GCs detected
2018-01-07 13:21:21,601 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 44013ms
No GCs detected
2018-01-07 13:21:25,538 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3436ms
No GCs detected
2018-01-07 13:21:47,558 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 5027ms
No GCs detected
2018-01-07 13:22:26,037 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 19942ms
No GCs detected
2018-01-07 13:22:38,560 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3012ms
No GCs detected
2018-01-07 13:22:45,973 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 6912ms
No GCs detected
2018-01-07 13:24:25,848 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 99374ms
No GCs detected
2018-01-07 13:24:28,491 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2142ms
No GCs detected
2018-01-07 13:24:30,149 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1158ms
No GCs detected
2018-01-07 13:24:40,271 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 9621ms
No GCs detected
2018-01-07 13:25:58,962 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 78190ms
No GCs detected
2018-01-07 13:26:04,559 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2925ms
No GCs detected
2018-01-07 13:26:10,357 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 5297ms
No GCs detected
2018-01-07 13:26:16,760 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 5903ms
No GCs detected
2018-01-07 13:26:23,487 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 6226ms
No GCs detected
2018-01-07 13:26:32,060 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 8073ms
No GCs detected
2018-01-07 13:27:54,706 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 82145ms
No GCs detected
2018-01-07 13:27:58,423 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3216ms
No GCs detected
2018-01-07 13:28:25,931 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1016ms
No GCs detected
2018-01-07 13:28:46,294 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4506ms
No GCs detected
2018-01-07 13:28:52,362 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 5563ms
No GCs detected
2018-01-07 13:29:00,602 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 7071ms
No GCs detected
2018-01-07 13:29:05,629 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4526ms
No GCs detected
2018-01-07 13:29:11,510 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1359ms
No GCs detected
2018-01-07 13:29:12,450 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 0 time(s); maxRetries=45
2018-01-07 13:31:32,792 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 140257ms
No GCs detected
2018-01-07 13:31:56,081 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 22789ms
No GCs detected
2018-01-07 13:32:05,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-07 13:32:12,217 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 15633ms
No GCs detected
2018-01-07 13:32:25,471 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 1 time(s); maxRetries=45
2018-01-07 13:33:10,571 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 28517ms
No GCs detected
2018-01-07 14:18:18,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker02.local/192.168.28.132
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-07 14:18:18,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-07 14:18:20,618 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-07 14:18:20,956 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-07 14:18:20,956 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-07 14:18:20,981 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-07 14:18:20,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker02.local
2018-01-07 14:18:21,006 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-07 14:18:21,163 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-07 14:18:21,167 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-07 14:18:21,167 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-07 14:18:21,800 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-07 14:18:21,867 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-07 14:18:21,888 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-07 14:18:21,905 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-07 14:18:21,909 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-07 14:18:21,910 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-07 14:18:21,910 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-07 14:18:21,957 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 45241
2018-01-07 14:18:21,957 INFO org.mortbay.log: jetty-6.1.26
2018-01-07 14:18:20,212 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:45241
2018-01-07 14:18:21,196 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-07 14:18:21,274 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-07 14:18:22,023 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-07 14:18:22,023 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-07 14:18:22,315 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-07 14:18:22,364 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-07 14:18:22,666 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-07 14:18:22,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-07 14:18:22,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-07 14:18:22,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-07 14:18:22,872 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-07 14:18:22,878 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-07 14:18:23,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-07 14:18:23,857 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-07 14:18:23,892 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 1788@hadoop-worker02.local
2018-01-07 14:18:23,980 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 1788@hadoop-worker02.local
2018-01-07 14:18:24,532 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-07 14:18:24,532 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-07 14:18:25,084 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-07 14:18:25,084 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-07 14:18:25,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=41fec607-6e2e-4e46-b768-7baf3ef5e9ef
2018-01-07 14:18:25,343 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ceff66d7-ed85-46b1-886b-6a380c086ba2
2018-01-07 14:18:25,343 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-07 14:18:25,347 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-17803564-4596-4115-a494-de0767943c31
2018-01-07 14:18:25,347 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-07 14:18:25,411 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-07 14:18:25,431 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-07 14:18:25,431 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-07 14:18:25,459 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-07 14:18:25,463 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-07 14:18:25,848 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 348ms
2018-01-07 14:18:25,927 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 427ms
2018-01-07 14:18:25,934 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 502ms
2018-01-07 14:18:25,948 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-07 14:18:25,949 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-07 14:18:26,053 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-07 14:18:26,053 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-07 14:18:26,063 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 10ms
2018-01-07 14:18:26,069 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 120ms
2018-01-07 14:18:26,069 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 127ms
2018-01-07 14:18:26,327 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-17803564-4596-4115-a494-de0767943c31): no suitable block pools found to scan.  Waiting 681817829 ms.
2018-01-07 14:18:26,328 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-ceff66d7-ed85-46b1-886b-6a380c086ba2): no suitable block pools found to scan.  Waiting 681817827 ms.
2018-01-07 14:18:26,359 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/7/18 4:58 PM with interval of 21600000ms
2018-01-07 14:18:26,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-07 14:18:26,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-07 14:18:26,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-07 14:18:26,901 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xa0ff1624bfefad59,  containing 2 storage report(s), of which we sent 2. The reports had 25 total blocks and used 1 RPC(s). This took 9 msec to generate and 167 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-07 14:18:26,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-07 14:21:09,847 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741899_1075 src: /192.168.28.131:50272 dest: /192.168.28.132:50010
2018-01-07 14:21:19,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:50272, dest: /192.168.28.132:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_353534513_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741899_1075, duration: 5863296445
2018-01-07 14:21:19,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741899_1075, type=LAST_IN_PIPELINE terminating
2018-01-07 14:21:20,000 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741900_1076 src: /192.168.28.131:50274 dest: /192.168.28.132:50010
2018-01-07 14:21:22,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:50274, dest: /192.168.28.132:50010, bytes: 75090821, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_353534513_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741900_1076, duration: 2771546165
2018-01-07 14:21:22,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741900_1076, type=LAST_IN_PIPELINE terminating
2018-01-07 14:21:23,091 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741901_1077 src: /192.168.28.130:45754 dest: /192.168.28.132:50010
2018-01-07 14:21:23,151 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:45754, dest: /192.168.28.132:50010, bytes: 66091, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_353534513_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741901_1077, duration: 15639935
2018-01-07 14:21:23,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741901_1077, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-07 16:13:22,495 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: "hadoop-worker02.local/192.168.28.132"; destination host is: "hadoop-master":8020; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:554)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-07 16:13:27,481 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-07 16:13:28,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-07 16:13:29,149 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-07 16:13:29,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker02.local/192.168.28.132
************************************************************/
2018-01-07 21:10:29,659 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker02.local/192.168.28.132
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-07 21:10:29,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-07 21:10:32,228 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-07 21:10:32,599 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-07 21:10:32,599 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-07 21:10:32,623 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-07 21:10:32,629 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker02.local
2018-01-07 21:10:32,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-07 21:10:32,724 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-07 21:10:32,729 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-07 21:10:32,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-07 21:10:33,375 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-07 21:10:33,539 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-07 21:10:33,558 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-07 21:10:33,573 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-07 21:10:33,577 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-07 21:10:33,577 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-07 21:10:33,577 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-07 21:10:33,666 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 37224
2018-01-07 21:10:33,667 INFO org.mortbay.log: jetty-6.1.26
2018-01-07 21:10:34,820 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:37224
2018-01-07 21:10:35,539 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-07 21:10:35,547 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-07 21:10:36,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-07 21:10:36,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-07 21:10:36,550 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-07 21:10:36,605 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-07 21:10:36,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-07 21:10:36,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-07 21:10:37,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-07 21:10:37,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-07 21:10:37,283 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-07 21:10:37,303 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-07 21:10:38,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-07 21:10:38,209 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-07 21:10:38,285 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 1749@hadoop-worker02.local
2018-01-07 21:10:38,326 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 1749@hadoop-worker02.local
2018-01-07 21:10:38,725 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-07 21:10:38,725 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-07 21:10:39,122 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-07 21:10:39,129 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-07 21:10:39,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=41fec607-6e2e-4e46-b768-7baf3ef5e9ef
2018-01-07 21:10:39,312 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ceff66d7-ed85-46b1-886b-6a380c086ba2
2018-01-07 21:10:39,312 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-07 21:10:39,316 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-17803564-4596-4115-a494-de0767943c31
2018-01-07 21:10:39,316 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-07 21:10:39,358 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-07 21:10:39,412 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-07 21:10:39,412 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-07 21:10:39,419 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-07 21:10:39,432 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-07 21:10:39,575 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 143ms
2018-01-07 21:10:39,591 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 158ms
2018-01-07 21:10:39,605 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 193ms
2018-01-07 21:10:39,623 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-07 21:10:39,624 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-07 21:10:39,632 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-07 21:10:39,632 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-07 21:10:39,677 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 45ms
2018-01-07 21:10:39,677 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 53ms
2018-01-07 21:10:39,678 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 69ms
2018-01-07 21:10:39,845 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-17803564-4596-4115-a494-de0767943c31): no suitable block pools found to scan.  Waiting 657084311 ms.
2018-01-07 21:10:39,858 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-ceff66d7-ed85-46b1-886b-6a380c086ba2): no suitable block pools found to scan.  Waiting 657084297 ms.
2018-01-07 21:10:39,959 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/8/18 2:54 AM with interval of 21600000ms
2018-01-07 21:10:39,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-07 21:10:40,149 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-07 21:10:40,149 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-07 21:10:40,538 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1b2e42c0114d9506,  containing 2 storage report(s), of which we sent 2. The reports had 28 total blocks and used 1 RPC(s). This took 11 msec to generate and 160 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-07 21:10:40,538 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-07 22:30:36,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1b2e42c0114d9507,  containing 2 storage report(s), of which we sent 2. The reports had 28 total blocks and used 1 RPC(s). This took 1 msec to generate and 12 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-07 22:30:36,407 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-07 23:35:18,496 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker02.local/192.168.28.132"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-07 23:35:22,495 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-07 23:35:22,834 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-07 23:35:22,844 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker02.local/192.168.28.132
************************************************************/
2018-01-08 07:14:10,141 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker02.local/192.168.28.132
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-08 07:14:10,163 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-08 07:14:12,422 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-08 07:14:12,796 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-08 07:14:12,796 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-08 07:14:12,816 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-08 07:14:12,819 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker02.local
2018-01-08 07:14:12,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-08 07:14:12,913 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-08 07:14:12,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-08 07:14:12,919 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-08 07:14:13,658 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-08 07:14:13,697 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-08 07:14:13,732 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-08 07:14:13,743 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-08 07:14:13,746 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-08 07:14:13,747 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-08 07:14:13,747 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-08 07:14:13,820 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 43627
2018-01-08 07:14:13,820 INFO org.mortbay.log: jetty-6.1.26
2018-01-08 07:14:14,576 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:43627
2018-01-08 07:14:15,618 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-08 07:14:15,629 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-08 07:14:16,634 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-08 07:14:16,634 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-08 07:14:16,921 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-08 07:14:16,983 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-08 07:14:17,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-08 07:14:17,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-08 07:14:17,236 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-08 07:14:17,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-08 07:14:17,357 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-08 07:14:17,365 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-08 07:14:17,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-08 07:14:17,961 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-08 07:14:18,064 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 1729@hadoop-worker02.local
2018-01-08 07:14:18,084 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 1729@hadoop-worker02.local
2018-01-08 07:14:18,383 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-08 07:14:18,568 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-08 07:14:19,180 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-08 07:14:19,180 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-08 07:14:19,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=41fec607-6e2e-4e46-b768-7baf3ef5e9ef
2018-01-08 07:14:19,396 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ceff66d7-ed85-46b1-886b-6a380c086ba2
2018-01-08 07:14:19,396 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-08 07:14:19,401 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-17803564-4596-4115-a494-de0767943c31
2018-01-08 07:14:19,401 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-08 07:14:19,423 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-08 07:14:19,456 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-08 07:14:19,456 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-08 07:14:19,476 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-08 07:14:19,512 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-08 07:14:19,647 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 161ms
2018-01-08 07:14:19,653 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 140ms
2018-01-08 07:14:19,653 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 197ms
2018-01-08 07:14:19,661 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-08 07:14:19,661 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-08 07:14:19,661 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-08 07:14:19,661 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-08 07:14:19,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 12ms
2018-01-08 07:14:19,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 12ms
2018-01-08 07:14:19,676 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 16ms
2018-01-08 07:14:19,815 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-17803564-4596-4115-a494-de0767943c31): no suitable block pools found to scan.  Waiting 620864340 ms.
2018-01-08 07:14:19,819 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-ceff66d7-ed85-46b1-886b-6a380c086ba2): no suitable block pools found to scan.  Waiting 620864336 ms.
2018-01-08 07:14:19,848 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/8/18 10:22 AM with interval of 21600000ms
2018-01-08 07:14:19,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-08 07:14:20,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-08 07:14:20,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-08 07:14:20,410 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x42f358aa19e38dc6,  containing 2 storage report(s), of which we sent 2. The reports had 28 total blocks and used 1 RPC(s). This took 15 msec to generate and 142 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-08 07:14:20,410 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-08 07:41:27,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741902_1078 src: /192.168.28.131:51512 dest: /192.168.28.132:50010
2018-01-08 07:41:34,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:51512, dest: /192.168.28.132:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1583035122_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741902_1078, duration: 6381123836
2018-01-08 07:41:34,212 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741902_1078, type=LAST_IN_PIPELINE terminating
2018-01-08 07:41:34,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741903_1079 src: /192.168.28.131:51514 dest: /192.168.28.132:50010
2018-01-08 07:41:36,801 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:51514, dest: /192.168.28.132:50010, bytes: 75090821, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1583035122_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741903_1079, duration: 2538261754
2018-01-08 07:41:36,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741903_1079, type=LAST_IN_PIPELINE terminating
2018-01-08 07:41:37,091 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741904_1080 src: /192.168.28.131:51516 dest: /192.168.28.132:50010
2018-01-08 07:41:37,110 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:51516, dest: /192.168.28.132:50010, bytes: 66091, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1583035122_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741904_1080, duration: 15746351
2018-01-08 07:41:37,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741904_1080, type=LAST_IN_PIPELINE terminating
2018-01-08 07:45:21,526 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker02.local/192.168.28.132"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-08 07:45:31,079 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-08 07:45:32,096 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-08 07:45:32,373 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-08 07:45:32,386 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker02.local/192.168.28.132
************************************************************/
2018-01-08 10:13:18,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker02.local/192.168.28.132
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-08 10:13:18,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-08 10:13:23,415 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-08 10:13:24,265 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-08 10:13:24,266 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-08 10:13:24,289 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-08 10:13:24,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker02.local
2018-01-08 10:13:24,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-08 10:13:24,407 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-08 10:13:24,412 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-08 10:13:24,412 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-08 10:13:25,985 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-08 10:13:26,091 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-08 10:13:26,121 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-08 10:13:26,135 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-08 10:13:26,139 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-08 10:13:26,139 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-08 10:13:26,139 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-08 10:13:26,211 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 45526
2018-01-08 10:13:26,211 INFO org.mortbay.log: jetty-6.1.26
2018-01-08 10:13:27,492 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:45526
2018-01-08 10:13:28,574 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-08 10:13:28,597 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-08 10:13:29,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-08 10:13:29,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-08 10:13:29,650 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-08 10:13:29,694 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-08 10:13:29,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-08 10:13:29,951 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-08 10:13:30,033 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-08 10:13:30,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-08 10:13:30,130 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-08 10:13:30,142 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-08 10:13:31,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-08 10:13:31,748 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-08 10:13:31,767 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 1789@hadoop-worker02.local
2018-01-08 10:13:31,835 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 1789@hadoop-worker02.local
2018-01-08 10:13:32,245 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-08 10:13:32,245 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-08 10:13:32,969 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-08 10:13:32,969 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-08 10:13:33,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=41fec607-6e2e-4e46-b768-7baf3ef5e9ef
2018-01-08 10:13:33,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ceff66d7-ed85-46b1-886b-6a380c086ba2
2018-01-08 10:13:33,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-08 10:13:33,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-17803564-4596-4115-a494-de0767943c31
2018-01-08 10:13:33,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-08 10:13:34,126 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-08 10:13:34,229 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-08 10:13:34,229 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-08 10:13:34,241 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-08 10:13:34,249 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-08 10:13:34,620 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 330ms
2018-01-08 10:13:34,629 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 343ms
2018-01-08 10:13:34,630 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 401ms
2018-01-08 10:13:34,635 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-08 10:13:34,635 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-08 10:13:34,639 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-08 10:13:34,639 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-08 10:13:34,702 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 67ms
2018-01-08 10:13:34,706 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 67ms
2018-01-08 10:13:34,706 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 73ms
2018-01-08 10:13:34,870 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-17803564-4596-4115-a494-de0767943c31): no suitable block pools found to scan.  Waiting 610109286 ms.
2018-01-08 10:13:34,895 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-ceff66d7-ed85-46b1-886b-6a380c086ba2): no suitable block pools found to scan.  Waiting 610109260 ms.
2018-01-08 10:13:34,973 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/8/18 1:23 PM with interval of 21600000ms
2018-01-08 10:13:35,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-08 10:13:35,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-08 10:13:35,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-08 10:13:36,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x241b16279320d9a2,  containing 2 storage report(s), of which we sent 2. The reports had 31 total blocks and used 1 RPC(s). This took 10 msec to generate and 203 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-08 10:13:36,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-08 10:19:22,163 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741891_1067 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741891 for deletion
2018-01-08 10:19:22,166 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741891_1067 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741891
2018-01-08 10:40:19,842 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741905_1081 src: /192.168.28.130:44760 dest: /192.168.28.132:50010
2018-01-08 10:40:20,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:44760, dest: /192.168.28.132:50010, bytes: 63213, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1759612168_1, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741905_1081, duration: 64650538
2018-01-08 10:40:20,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741905_1081, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-08 10:40:58,621 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741905_1081 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741905 for deletion
2018-01-08 10:40:58,624 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741905_1081 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741905
2018-01-08 10:41:23,439 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741906_1082 src: /192.168.28.131:52904 dest: /192.168.28.132:50010
2018-01-08 10:41:23,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:52904, dest: /192.168.28.132:50010, bytes: 63213, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1759612168_1, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741906_1082, duration: 14739651
2018-01-08 10:41:23,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741906_1082, type=LAST_IN_PIPELINE terminating
2018-01-08 10:42:30,938 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741907_1083 src: /192.168.28.130:44952 dest: /192.168.28.132:50010
2018-01-08 10:42:36,111 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:44952, dest: /192.168.28.132:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-22806767_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741907_1083, duration: 5165102394
2018-01-08 10:42:36,111 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741907_1083, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-08 10:42:36,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741908_1084 src: /192.168.28.131:52906 dest: /192.168.28.132:50010
2018-01-08 10:42:39,138 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:52906, dest: /192.168.28.132:50010, bytes: 75090821, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-22806767_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741908_1084, duration: 3002686440
2018-01-08 10:42:39,138 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741908_1084, type=LAST_IN_PIPELINE terminating
2018-01-08 10:42:39,919 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741909_1085 src: /192.168.28.130:44956 dest: /192.168.28.132:50010
2018-01-08 10:42:39,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:44956, dest: /192.168.28.132:50010, bytes: 66091, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-22806767_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741909_1085, duration: 18033495
2018-01-08 10:42:39,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741909_1085, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-08 10:43:12,936 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4453ms
No GCs detected
2018-01-08 10:52:37,592 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741907_1083 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741907 for deletion
2018-01-08 10:52:37,603 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741908_1084 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741908 for deletion
2018-01-08 10:52:37,619 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741909_1085 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741909 for deletion
2018-01-08 10:52:37,625 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741907_1083 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741907
2018-01-08 10:52:37,627 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741909_1085 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741909
2018-01-08 10:52:37,632 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741908_1084 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741908
2018-01-08 10:58:07,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741910_1086 src: /192.168.28.131:52942 dest: /192.168.28.132:50010
2018-01-08 10:58:10,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:52942, dest: /192.168.28.132:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_11353863_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741910_1086, duration: 3521176296
2018-01-08 10:58:10,619 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741910_1086, type=LAST_IN_PIPELINE terminating
2018-01-08 10:58:10,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741911_1087 src: /192.168.28.130:45158 dest: /192.168.28.132:50010
2018-01-08 10:58:13,131 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:45158, dest: /192.168.28.132:50010, bytes: 75090821, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_11353863_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741911_1087, duration: 2480787798
2018-01-08 10:58:13,131 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741911_1087, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-08 10:58:13,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741912_1088 src: /192.168.28.130:45160 dest: /192.168.28.132:50010
2018-01-08 10:58:13,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:45160, dest: /192.168.28.132:50010, bytes: 66091, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_11353863_12, offset: 0, srvID: 41fec607-6e2e-4e46-b768-7baf3ef5e9ef, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741912_1088, duration: 8498645
2018-01-08 10:58:13,436 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741912_1088, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-08 11:12:19,917 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741910_1086 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741910 for deletion
2018-01-08 11:12:19,924 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741911_1087 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741911 for deletion
2018-01-08 11:12:19,927 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741912_1088 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741912 for deletion
2018-01-08 11:12:19,933 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741911_1087 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741911
2018-01-08 11:12:19,934 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741910_1086 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741910
2018-01-08 11:12:19,935 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741912_1088 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741912
2018-01-08 11:13:07,076 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker02.local/192.168.28.132"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-08 11:13:10,135 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-08 11:13:10,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker02.local/192.168.28.132
************************************************************/

2018-01-11 11:42:48,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker03.local/192.168.28.134
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-11 11:42:48,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-11 11:42:52,136 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-11 11:42:52,626 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-11 11:42:52,626 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-11 11:42:52,645 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-11 11:42:52,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker03.local
2018-01-11 11:42:52,657 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-11 11:42:52,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-11 11:42:52,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-11 11:42:52,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-11 11:42:53,510 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-11 11:42:53,802 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-11 11:42:53,953 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-11 11:42:53,962 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-11 11:42:53,964 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-11 11:42:53,965 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-11 11:42:53,965 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-11 11:42:54,047 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 35538
2018-01-11 11:42:54,048 INFO org.mortbay.log: jetty-6.1.26
2018-01-11 11:42:55,176 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:35538
2018-01-11 11:42:56,001 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-11 11:42:56,015 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-11 11:42:57,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-11 11:42:57,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-11 11:42:57,457 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-11 11:42:57,512 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-11 11:42:57,903 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-11 11:42:57,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-11 11:42:58,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-11 11:42:58,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-11 11:42:58,155 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-11 11:42:58,174 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-11 11:42:59,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-11 11:42:59,962 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-11 11:42:59,991 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 1945@hadoop-worker03.local
2018-01-11 11:43:00,161 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 1945@hadoop-worker03.local
2018-01-11 11:43:00,638 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:43:00,638 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:43:01,452 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:43:01,452 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:43:01,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=41fec607-6e2e-4e46-b768-7baf3ef5e9ef
2018-01-11 11:43:01,741 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ceff66d7-ed85-46b1-886b-6a380c086ba2
2018-01-11 11:43:01,742 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-11 11:43:01,749 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-17803564-4596-4115-a494-de0767943c31
2018-01-11 11:43:01,749 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-11 11:43:01,847 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-11 11:43:01,911 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-11 11:43:01,911 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:43:01,914 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-11 11:43:01,918 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-11 11:43:01,997 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 79ms
2018-01-11 11:43:02,005 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 86ms
2018-01-11 11:43:02,005 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 93ms
2018-01-11 11:43:02,010 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-11 11:43:02,010 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-11 11:43:02,010 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-11 11:43:02,010 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-11 11:43:02,026 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 16ms
2018-01-11 11:43:02,028 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 18ms
2018-01-11 11:43:02,032 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 24ms
2018-01-11 11:43:02,215 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-17803564-4596-4115-a494-de0767943c31): no suitable block pools found to scan.  Waiting 345541940 ms.
2018-01-11 11:43:02,295 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-ceff66d7-ed85-46b1-886b-6a380c086ba2): no suitable block pools found to scan.  Waiting 345541860 ms.
2018-01-11 11:43:02,335 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/11/18 1:06 PM with interval of 21600000ms
2018-01-11 11:43:02,611 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-11 11:43:02,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-11 11:43:02,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-11 11:43:03,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3b8e9cc9995296fc,  containing 2 storage report(s), of which we sent 2. The reports had 31 total blocks and used 1 RPC(s). This took 7 msec to generate and 187 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-11 11:43:03,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:43:05,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from hadoop-master/192.168.28.129:8020 with active state
2018-01-11 11:43:05,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-11 11:43:05,891 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-11 11:43:05,908 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3b8e9cc9995296fd,  containing 2 storage report(s), of which we sent 2. The reports had 31 total blocks and used 1 RPC(s). This took 0 msec to generate and 11 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-11 11:43:05,908 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:43:09,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from hadoop-master/192.168.28.129:8020 with active state
2018-01-11 11:43:09,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-11 11:43:09,212 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-11 11:43:09,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3b8e9cc9995296fe,  containing 2 storage report(s), of which we sent 2. The reports had 31 total blocks and used 1 RPC(s). This took 0 msec to generate and 53 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-11 11:43:09,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:43:12,411 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from hadoop-master/192.168.28.129:8020 with active state
2018-01-11 11:43:12,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-11 11:43:12,682 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-11 11:43:13,351 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3b8e9cc9995296ff,  containing 2 storage report(s), of which we sent 2. The reports had 31 total blocks and used 1 RPC(s). This took 0 msec to generate and 144 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-11 11:43:13,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:43:19,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from hadoop-master/192.168.28.129:8020 with active state
2018-01-11 11:43:19,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-11 11:43:19,163 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-11 11:43:19,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Unsuccessfully sent block report 0x3b8e9cc999529700,  containing 2 storage report(s), of which we sent 0. The reports had 31 total blocks and used 0 RPC(s). This took 1 msec to generate and 44 msecs for RPC and NN processing. Got back no commands.
2018-01-11 11:43:19,225 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020 is shutting down
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.UnregisteredNodeException): Data node DatanodeRegistration(192.168.28.134:50010, datanodeUuid=41fec607-6e2e-4e46-b768-7baf3ef5e9ef, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712) is attempting to report storage ID 41fec607-6e2e-4e46-b768-7baf3ef5e9ef. Node 192.168.28.132:50010 is expected to serve this storage.
	at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.getDatanode(DatanodeManager.java:509)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:1969)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer$1.call(NameNodeRpcServer.java:1426)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer$1.call(NameNodeRpcServer.java:1423)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockReportProcessingThread.processQueue(BlockManager.java:4018)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockReportProcessingThread.run(BlockManager.java:3997)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1483)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.blockReport(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.blockReport(DatanodeProtocolClientSideTranslatorPB.java:203)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.blockReport(BPServiceActor.java:362)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:633)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
2018-01-11 11:43:19,229 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef) service to hadoop-master/192.168.28.129:8020
2018-01-11 11:43:19,349 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 41fec607-6e2e-4e46-b768-7baf3ef5e9ef)
2018-01-11 11:43:19,349 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Removing block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:43:19,374 WARN org.apache.hadoop.fs.CachingGetSpaceUsed: Thread Interrupted waiting to refresh disk information: sleep interrupted
2018-01-11 11:43:19,389 WARN org.apache.hadoop.fs.CachingGetSpaceUsed: Thread Interrupted waiting to refresh disk information: sleep interrupted
2018-01-11 11:43:21,377 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2018-01-11 11:43:21,379 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2018-01-11 11:43:21,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker03.local/192.168.28.134
************************************************************/
2018-01-11 11:50:17,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker03.local/192.168.28.134
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-11 11:50:17,506 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-11 11:50:19,981 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-11 11:50:20,199 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-11 11:50:20,199 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-11 11:50:20,210 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-11 11:50:20,212 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker03.local
2018-01-11 11:50:20,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-11 11:50:20,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-11 11:50:20,358 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-11 11:50:20,358 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-11 11:50:21,136 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-11 11:50:21,153 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-11 11:50:21,207 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-11 11:50:21,216 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-11 11:50:21,219 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-11 11:50:21,219 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-11 11:50:21,219 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-11 11:50:21,252 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 38204
2018-01-11 11:50:21,252 INFO org.mortbay.log: jetty-6.1.26
2018-01-11 11:50:22,051 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:38204
2018-01-11 11:50:23,108 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-11 11:50:23,208 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-11 11:50:24,441 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-11 11:50:24,441 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-11 11:50:24,802 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-11 11:50:24,850 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-11 11:50:25,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-11 11:50:25,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-11 11:50:25,308 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-11 11:50:25,361 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-11 11:50:25,422 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-11 11:50:25,426 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-11 11:50:26,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-11 11:50:26,769 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-11 11:50:26,872 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 2613@hadoop-worker03.local
2018-01-11 11:50:26,874 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /var/lib/hadoop2/dfs/data/1 is not formatted for namespace 1794692519. Formatting...
2018-01-11 11:50:26,885 INFO org.apache.hadoop.hdfs.server.common.Storage: Generated new storageID DS-7db69c5d-4796-444d-88f0-25ee3c547483 for directory /var/lib/hadoop2/dfs/data/1
2018-01-11 11:50:26,950 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 2613@hadoop-worker03.local
2018-01-11 11:50:26,950 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /var/lib/hadoop2/dfs/data/2 is not formatted for namespace 1794692519. Formatting...
2018-01-11 11:50:26,951 INFO org.apache.hadoop.hdfs.server.common.Storage: Generated new storageID DS-9f9e4819-9863-4a6c-9ad1-cfd0eedcc866 for directory /var/lib/hadoop2/dfs/data/2
2018-01-11 11:50:27,285 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:50:27,286 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:50:27,286 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712 is not formatted for BP-199680249-192.168.28.129-1514169681712. Formatting ...
2018-01-11 11:50:27,286 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-199680249-192.168.28.129-1514169681712 directory /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current
2018-01-11 11:50:27,424 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:50:27,424 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:50:27,424 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712 is not formatted for BP-199680249-192.168.28.129-1514169681712. Formatting ...
2018-01-11 11:50:27,424 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-199680249-192.168.28.129-1514169681712 directory /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current
2018-01-11 11:50:27,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=null
2018-01-11 11:50:27,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID bb921c4e-2a2d-4f68-a9b7-35e17bf11977
2018-01-11 11:50:28,203 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-7db69c5d-4796-444d-88f0-25ee3c547483
2018-01-11 11:50:28,204 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-11 11:50:28,223 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-9f9e4819-9863-4a6c-9ad1-cfd0eedcc866
2018-01-11 11:50:28,223 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-11 11:50:28,308 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-11 11:50:28,318 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-11 11:50:28,318 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:50:28,320 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-11 11:50:28,364 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-11 11:50:28,719 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 379ms
2018-01-11 11:50:28,785 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 420ms
2018-01-11 11:50:28,794 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 477ms
2018-01-11 11:50:28,850 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-11 11:50:28,850 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-11 11:50:28,850 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 1ms
2018-01-11 11:50:29,010 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-11 11:50:29,011 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-11 11:50:29,052 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 41ms
2018-01-11 11:50:29,059 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 250ms
2018-01-11 11:50:29,093 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2
2018-01-11 11:50:29,095 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-9f9e4819-9863-4a6c-9ad1-cfd0eedcc866): finished scanning block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:50:29,100 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1
2018-01-11 11:50:29,231 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-7db69c5d-4796-444d-88f0-25ee3c547483): finished scanning block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:50:29,262 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/11/18 3:31 PM with interval of 21600000ms
2018-01-11 11:50:29,305 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-9f9e4819-9863-4a6c-9ad1-cfd0eedcc866): no suitable block pools found to scan.  Waiting 1814399757 ms.
2018-01-11 11:50:29,322 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-7db69c5d-4796-444d-88f0-25ee3c547483): no suitable block pools found to scan.  Waiting 1814399739 ms.
2018-01-11 11:50:29,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid bb921c4e-2a2d-4f68-a9b7-35e17bf11977) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-11 11:50:29,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid bb921c4e-2a2d-4f68-a9b7-35e17bf11977) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-11 11:50:29,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-11 11:50:30,319 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x31e38e06969add6b,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 36 msec to generate and 229 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-11 11:50:30,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:51:03,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741897_1073 src: /192.168.28.131:43710 dest: /192.168.28.134:50010
2018-01-11 11:51:03,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741895_1071 src: /192.168.28.131:43708 dest: /192.168.28.134:50010
2018-01-11 11:51:03,446 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741895_1071 src: /192.168.28.131:43708 dest: /192.168.28.134:50010 of size 399
2018-01-11 11:51:04,877 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741897_1073 src: /192.168.28.131:43710 dest: /192.168.28.134:50010 of size 75090821
2018-01-11 11:51:05,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741831_1007 src: /192.168.28.132:51958 dest: /192.168.28.134:50010
2018-01-11 11:51:05,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741833_1009 src: /192.168.28.132:51960 dest: /192.168.28.134:50010
2018-01-11 11:51:05,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741831_1007 src: /192.168.28.132:51958 dest: /192.168.28.134:50010 of size 239
2018-01-11 11:51:05,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741833_1009 src: /192.168.28.132:51960 dest: /192.168.28.134:50010 of size 1337
2018-01-11 11:51:06,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741901_1077 src: /192.168.28.131:43712 dest: /192.168.28.134:50010
2018-01-11 11:51:06,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741901_1077 src: /192.168.28.131:43712 dest: /192.168.28.134:50010 of size 66091
2018-01-11 11:51:06,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741839_1015 src: /192.168.28.131:43714 dest: /192.168.28.134:50010
2018-01-11 11:51:06,140 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741839_1015 src: /192.168.28.131:43714 dest: /192.168.28.134:50010 of size 260
2018-01-11 11:51:07,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741837_1013 src: /192.168.28.132:51962 dest: /192.168.28.134:50010
2018-01-11 11:51:07,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741902_1078 src: /192.168.28.132:51964 dest: /192.168.28.134:50010
2018-01-11 11:51:07,732 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741837_1013 src: /192.168.28.132:51962 dest: /192.168.28.134:50010 of size 1338
2018-01-11 11:51:09,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741844_1020 src: /192.168.28.131:43716 dest: /192.168.28.134:50010
2018-01-11 11:51:09,187 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741843_1019 src: /192.168.28.131:43718 dest: /192.168.28.134:50010
2018-01-11 11:51:09,187 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741844_1020 src: /192.168.28.131:43716 dest: /192.168.28.134:50010 of size 30
2018-01-11 11:51:09,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741843_1019 src: /192.168.28.131:43718 dest: /192.168.28.134:50010 of size 271
2018-01-11 11:51:09,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741902_1078 src: /192.168.28.132:51964 dest: /192.168.28.134:50010 of size 134217728
2018-01-11 11:51:10,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741850_1026 src: /192.168.28.132:51968 dest: /192.168.28.134:50010
2018-01-11 11:51:10,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741848_1024 src: /192.168.28.132:51966 dest: /192.168.28.134:50010
2018-01-11 11:51:10,434 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741850_1026 src: /192.168.28.132:51968 dest: /192.168.28.134:50010 of size 65912
2018-01-11 11:51:11,845 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741848_1024 src: /192.168.28.132:51966 dest: /192.168.28.134:50010 of size 134217728
2018-01-11 11:51:12,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741832_1008 src: /192.168.28.131:43720 dest: /192.168.28.134:50010
2018-01-11 11:51:12,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741834_1010 src: /192.168.28.131:43722 dest: /192.168.28.134:50010
2018-01-11 11:51:12,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741832_1008 src: /192.168.28.131:43720 dest: /192.168.28.134:50010 of size 1330
2018-01-11 11:51:12,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741834_1010 src: /192.168.28.131:43722 dest: /192.168.28.134:50010 of size 1338
2018-01-11 11:51:13,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741898_1074 src: /192.168.28.132:51972 dest: /192.168.28.134:50010
2018-01-11 11:51:13,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741896_1072 src: /192.168.28.132:51970 dest: /192.168.28.134:50010
2018-01-11 11:51:13,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741898_1074 src: /192.168.28.132:51972 dest: /192.168.28.134:50010 of size 66091
2018-01-11 11:51:14,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741896_1072 src: /192.168.28.132:51970 dest: /192.168.28.134:50010 of size 134217728
2018-01-11 11:51:15,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741840_1016 src: /192.168.28.131:43726 dest: /192.168.28.134:50010
2018-01-11 11:51:15,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741838_1014 src: /192.168.28.131:43724 dest: /192.168.28.134:50010
2018-01-11 11:51:15,135 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741838_1014 src: /192.168.28.131:43724 dest: /192.168.28.134:50010 of size 260
2018-01-11 11:51:15,138 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741840_1016 src: /192.168.28.131:43726 dest: /192.168.28.134:50010 of size 271
2018-01-11 11:51:15,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741906_1082 src: /192.168.28.132:51976 dest: /192.168.28.134:50010
2018-01-11 11:51:15,828 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741906_1082 src: /192.168.28.132:51976 dest: /192.168.28.134:50010 of size 63213
2018-01-11 11:51:15,828 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741903_1079 src: /192.168.28.132:51974 dest: /192.168.28.134:50010
2018-01-11 11:51:16,539 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741903_1079 src: /192.168.28.132:51974 dest: /192.168.28.134:50010 of size 75090821
2018-01-11 11:51:18,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741882_1058 src: /192.168.28.132:51978 dest: /192.168.28.134:50010
2018-01-11 11:51:19,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741882_1058 src: /192.168.28.132:51978 dest: /192.168.28.134:50010 of size 75090821
2018-01-11 11:51:20,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741841_1017 src: /192.168.28.131:43730 dest: /192.168.28.134:50010
2018-01-11 11:51:20,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741835_1011 src: /192.168.28.131:43728 dest: /192.168.28.134:50010
2018-01-11 11:51:20,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741841_1017 src: /192.168.28.131:43730 dest: /192.168.28.134:50010 of size 1362
2018-01-11 11:51:20,941 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741835_1011 src: /192.168.28.131:43728 dest: /192.168.28.134:50010 of size 1338
2018-01-11 11:51:21,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741904_1080 src: /192.168.28.132:51982 dest: /192.168.28.134:50010
2018-01-11 11:51:21,726 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741899_1075 src: /192.168.28.132:51980 dest: /192.168.28.134:50010
2018-01-11 11:51:21,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741904_1080 src: /192.168.28.132:51982 dest: /192.168.28.134:50010 of size 66091
2018-01-11 11:51:22,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741899_1075 src: /192.168.28.132:51980 dest: /192.168.28.134:50010 of size 134217728
2018-01-11 11:51:23,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741836_1012 src: /192.168.28.131:43732 dest: /192.168.28.134:50010
2018-01-11 11:51:23,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741836_1012 src: /192.168.28.131:43732 dest: /192.168.28.134:50010 of size 1338
2018-01-11 11:51:23,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741842_1018 src: /192.168.28.131:43734 dest: /192.168.28.134:50010
2018-01-11 11:51:23,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741842_1018 src: /192.168.28.131:43734 dest: /192.168.28.134:50010 of size 271
2018-01-11 11:51:24,734 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741849_1025 src: /192.168.28.132:51986 dest: /192.168.28.134:50010
2018-01-11 11:51:24,735 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741900_1076 src: /192.168.28.132:51984 dest: /192.168.28.134:50010
2018-01-11 11:51:25,842 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741900_1076 src: /192.168.28.132:51984 dest: /192.168.28.134:50010 of size 75090821
2018-01-11 11:51:25,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741849_1025 src: /192.168.28.132:51986 dest: /192.168.28.134:50010 of size 74090694
2018-01-11 11:51:27,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741883_1059 src: /192.168.28.132:51988 dest: /192.168.28.134:50010
2018-01-11 11:51:27,727 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741883_1059 src: /192.168.28.132:51988 dest: /192.168.28.134:50010 of size 66101
2018-01-11 11:51:29,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741881_1057 src: /192.168.28.131:43736 dest: /192.168.28.134:50010
2018-01-11 11:51:30,161 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-199680249-192.168.28.129-1514169681712:blk_1073741881_1057 src: /192.168.28.131:43736 dest: /192.168.28.134:50010 of size 134217728
2018-01-11 12:43:16,880 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker03.local/192.168.28.134"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-11 12:43:20,880 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-11 12:43:21,776 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-11 12:43:21,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker03.local/192.168.28.134
************************************************************/
2018-01-11 16:22:30,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker03.local/192.168.28.134
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-11 16:22:30,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-11 16:22:34,237 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-11 16:22:34,700 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-11 16:22:34,700 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-11 16:22:34,713 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-11 16:22:34,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker03.local
2018-01-11 16:22:34,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-11 16:22:34,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-11 16:22:34,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-11 16:22:34,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-11 16:22:35,581 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-11 16:22:35,660 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-11 16:22:35,684 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-11 16:22:35,719 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-11 16:22:35,739 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-11 16:22:35,739 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-11 16:22:35,739 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-11 16:22:35,861 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 42489
2018-01-11 16:22:35,861 INFO org.mortbay.log: jetty-6.1.26
2018-01-11 16:22:36,898 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:42489
2018-01-11 16:22:38,012 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-11 16:22:38,018 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-11 16:22:38,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-11 16:22:38,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-11 16:22:42,168 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-11 16:22:42,231 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-11 16:22:42,565 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-11 16:22:42,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-11 16:22:42,742 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-11 16:22:42,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-11 16:22:43,018 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-11 16:22:43,020 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-11 16:22:44,660 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-11 16:22:44,675 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-11 16:22:44,693 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 1800@hadoop-worker03.local
2018-01-11 16:22:44,755 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 1800@hadoop-worker03.local
2018-01-11 16:22:45,215 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-11 16:22:45,215 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-11 16:22:45,783 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-11 16:22:45,783 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-11 16:22:45,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=bb921c4e-2a2d-4f68-a9b7-35e17bf11977
2018-01-11 16:22:46,279 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-7db69c5d-4796-444d-88f0-25ee3c547483
2018-01-11 16:22:46,291 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-11 16:22:46,311 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-9f9e4819-9863-4a6c-9ad1-cfd0eedcc866
2018-01-11 16:22:46,311 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-11 16:22:46,349 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-11 16:22:46,848 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-11 16:22:46,849 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 16:22:47,005 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-11 16:22:46,872 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-11 16:22:47,462 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 448ms
2018-01-11 16:22:47,462 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 456ms
2018-01-11 16:22:47,462 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 614ms
2018-01-11 16:22:47,479 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-11 16:22:47,480 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-11 16:22:47,544 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-11 16:22:47,554 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 75ms
2018-01-11 16:22:47,545 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-11 16:22:47,718 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 173ms
2018-01-11 16:22:47,745 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 269ms
2018-01-11 16:22:47,892 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-7db69c5d-4796-444d-88f0-25ee3c547483): no suitable block pools found to scan.  Waiting 1798061169 ms.
2018-01-11 16:22:47,913 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-9f9e4819-9863-4a6c-9ad1-cfd0eedcc866): no suitable block pools found to scan.  Waiting 1798061149 ms.
2018-01-11 16:22:47,946 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/11/18 6:04 PM with interval of 21600000ms
2018-01-11 16:22:48,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid bb921c4e-2a2d-4f68-a9b7-35e17bf11977) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-11 16:22:48,383 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid bb921c4e-2a2d-4f68-a9b7-35e17bf11977) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-11 16:22:48,383 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-11 16:22:49,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xe85431ef4b47128b,  containing 2 storage report(s), of which we sent 2. The reports had 31 total blocks and used 1 RPC(s). This took 10 msec to generate and 238 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-11 16:22:49,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 16:23:15,356 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1053ms
No GCs detected
2018-01-11 16:28:06,914 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker03.local/192.168.28.134"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-11 16:28:10,916 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-11 16:28:11,918 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-11 16:28:12,108 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-11 16:28:12,125 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker03.local/192.168.28.134
************************************************************/
2018-01-11 17:22:05,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker03.local/192.168.28.134
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-11 17:22:05,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-11 17:22:08,483 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-11 17:22:08,803 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-11 17:22:08,803 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-11 17:22:08,844 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-11 17:22:08,901 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker03.local
2018-01-11 17:22:08,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-11 17:22:09,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-11 17:22:09,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-11 17:22:09,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-11 17:22:10,032 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-11 17:22:10,056 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-11 17:22:10,117 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-11 17:22:10,129 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-11 17:22:10,131 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-11 17:22:10,132 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-11 17:22:10,132 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-11 17:22:10,201 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 34991
2018-01-11 17:22:10,201 INFO org.mortbay.log: jetty-6.1.26
2018-01-11 17:22:11,467 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:34991
2018-01-11 17:22:12,443 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-11 17:22:12,492 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-11 17:22:13,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-11 17:22:13,443 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-11 17:22:13,629 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-11 17:22:13,686 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-11 17:22:13,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-11 17:22:13,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-11 17:22:14,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-11 17:22:14,038 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-11 17:22:14,161 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-11 17:22:14,173 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-11 17:22:15,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-11 17:22:15,640 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-11 17:22:15,740 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 2547@hadoop-worker03.local
2018-01-11 17:22:15,792 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 2547@hadoop-worker03.local
2018-01-11 17:22:16,423 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-11 17:22:16,424 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-11 17:22:17,130 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-11 17:22:17,131 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-11 17:22:17,153 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=bb921c4e-2a2d-4f68-a9b7-35e17bf11977
2018-01-11 17:22:17,626 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-7db69c5d-4796-444d-88f0-25ee3c547483
2018-01-11 17:22:17,626 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-11 17:22:17,686 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-9f9e4819-9863-4a6c-9ad1-cfd0eedcc866
2018-01-11 17:22:17,686 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-11 17:22:17,819 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-11 17:22:17,894 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-11 17:22:17,894 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 17:22:17,915 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-11 17:22:17,916 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-11 17:22:18,381 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 447ms
2018-01-11 17:22:18,407 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 491ms
2018-01-11 17:22:18,408 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 514ms
2018-01-11 17:22:18,413 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-11 17:22:18,413 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-11 17:22:18,428 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-11 17:22:18,429 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-11 17:22:18,436 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 8ms
2018-01-11 17:22:18,442 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 30ms
2018-01-11 17:22:18,443 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 31ms
2018-01-11 17:22:18,552 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-9f9e4819-9863-4a6c-9ad1-cfd0eedcc866): no suitable block pools found to scan.  Waiting 1794490510 ms.
2018-01-11 17:22:18,554 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-7db69c5d-4796-444d-88f0-25ee3c547483): no suitable block pools found to scan.  Waiting 1794490507 ms.
2018-01-11 17:22:18,711 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/11/18 10:28 PM with interval of 21600000ms
2018-01-11 17:22:18,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid bb921c4e-2a2d-4f68-a9b7-35e17bf11977) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-11 17:22:19,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid bb921c4e-2a2d-4f68-a9b7-35e17bf11977) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-11 17:22:19,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-11 17:22:19,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x4c5a9e98544a4e63,  containing 2 storage report(s), of which we sent 2. The reports had 31 total blocks and used 1 RPC(s). This took 13 msec to generate and 245 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-11 17:22:19,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 17:28:39,558 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker03.local/192.168.28.134"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-11 17:28:43,558 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-11 17:28:44,174 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-11 17:28:44,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker03.local/192.168.28.134
************************************************************/
2018-01-11 17:34:49,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker03.local/192.168.28.134
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-11 17:34:49,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-11 17:34:52,913 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-11 17:34:53,320 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-11 17:34:53,320 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-11 17:34:53,333 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-11 17:34:53,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker03.local
2018-01-11 17:34:53,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-11 17:34:53,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-11 17:34:53,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-11 17:34:53,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-11 17:34:54,478 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-11 17:34:54,533 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-11 17:34:54,579 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-11 17:34:54,588 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-11 17:34:54,590 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-11 17:34:54,590 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-11 17:34:54,590 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-11 17:34:54,637 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 42028
2018-01-11 17:34:54,637 INFO org.mortbay.log: jetty-6.1.26
2018-01-11 17:34:55,498 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:42028
2018-01-11 17:34:56,626 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-11 17:34:56,778 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-11 17:34:57,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-11 17:34:57,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-11 17:34:57,882 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-11 17:34:57,933 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-11 17:34:58,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-11 17:34:58,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-11 17:34:58,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-11 17:34:58,253 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-11 17:34:58,380 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-11 17:34:58,385 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-11 17:34:59,238 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-11 17:34:59,286 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-11 17:34:59,325 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 1735@hadoop-worker03.local
2018-01-11 17:34:59,389 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 1735@hadoop-worker03.local
2018-01-11 17:34:59,888 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-11 17:34:59,888 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-11 17:35:00,347 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-11 17:35:00,347 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-11 17:35:00,349 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=bb921c4e-2a2d-4f68-a9b7-35e17bf11977
2018-01-11 17:35:00,724 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-7db69c5d-4796-444d-88f0-25ee3c547483
2018-01-11 17:35:00,724 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-11 17:35:00,730 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-9f9e4819-9863-4a6c-9ad1-cfd0eedcc866
2018-01-11 17:35:00,730 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-11 17:35:00,761 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-11 17:35:00,779 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-11 17:35:00,779 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 17:35:00,781 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-11 17:35:00,782 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-11 17:35:00,856 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current: 422277120
2018-01-11 17:35:00,857 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current: 632070144
2018-01-11 17:35:00,861 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 78ms
2018-01-11 17:35:00,874 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 91ms
2018-01-11 17:35:00,874 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 95ms
2018-01-11 17:35:00,896 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-11 17:35:00,896 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-11 17:35:00,900 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-11 17:35:00,900 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-11 17:35:00,997 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 97ms
2018-01-11 17:35:01,034 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 137ms
2018-01-11 17:35:01,034 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 140ms
2018-01-11 17:35:01,176 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-7db69c5d-4796-444d-88f0-25ee3c547483): no suitable block pools found to scan.  Waiting 1793727885 ms.
2018-01-11 17:35:01,188 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-9f9e4819-9863-4a6c-9ad1-cfd0eedcc866): no suitable block pools found to scan.  Waiting 1793727874 ms.
2018-01-11 17:35:01,246 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/11/18 6:00 PM with interval of 21600000ms
2018-01-11 17:35:01,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid bb921c4e-2a2d-4f68-a9b7-35e17bf11977) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-11 17:35:01,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid bb921c4e-2a2d-4f68-a9b7-35e17bf11977) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-11 17:35:01,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-11 17:35:01,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x4da98d02f06ee052,  containing 2 storage report(s), of which we sent 2. The reports had 31 total blocks and used 1 RPC(s). This took 11 msec to generate and 82 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-11 17:35:01,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 17:36:55,567 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1208ms
No GCs detected
2018-01-11 17:36:57,215 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1147ms
No GCs detected
2018-01-11 17:36:59,672 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1957ms
No GCs detected
2018-01-11 17:42:54,862 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741913_1089 src: /192.168.28.132:33006 dest: /192.168.28.134:50010
2018-01-11 17:42:55,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:33006, dest: /192.168.28.134:50010, bytes: 63213, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_358121713_50, offset: 0, srvID: bb921c4e-2a2d-4f68-a9b7-35e17bf11977, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741913_1089, duration: 517374844
2018-01-11 17:42:55,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741913_1089, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-11 17:46:24,058 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741906_1082 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741906 for deletion
2018-01-11 17:46:24,060 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741906_1082 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741906
2018-01-11 17:46:30,050 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741844_1020 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741844 for deletion
2018-01-11 17:46:30,051 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741844_1020 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741844
2018-01-11 17:47:21,084 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker03.local/192.168.28.134"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-11 17:47:25,086 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-11 17:47:25,962 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-11 17:47:25,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker03.local/192.168.28.134
************************************************************/
2018-01-11 18:49:15,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker03.local/192.168.28.134
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-11 18:49:15,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-11 18:49:17,492 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-11 18:49:17,639 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-11 18:49:17,639 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-11 18:49:17,660 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-11 18:49:17,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker03.local
2018-01-11 18:49:17,667 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-11 18:49:17,792 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-11 18:49:17,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-11 18:49:17,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-11 18:49:18,244 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-11 18:49:18,346 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-11 18:49:18,366 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-11 18:49:18,376 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-11 18:49:18,379 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-11 18:49:18,379 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-11 18:49:18,379 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-11 18:49:18,416 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 39423
2018-01-11 18:49:18,416 INFO org.mortbay.log: jetty-6.1.26
2018-01-11 18:49:19,486 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:39423
2018-01-11 18:49:20,634 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-11 18:49:20,652 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-11 18:49:21,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-11 18:49:21,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-11 18:49:22,026 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-11 18:49:22,067 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-11 18:49:22,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-11 18:49:22,383 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-11 18:49:22,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-11 18:49:22,475 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-11 18:49:22,528 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-11 18:49:22,549 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-11 18:49:23,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-11 18:49:23,819 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-11 18:49:23,848 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 2492@hadoop-worker03.local
2018-01-11 18:49:23,895 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 2492@hadoop-worker03.local
2018-01-11 18:49:24,286 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-11 18:49:24,286 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-11 18:49:24,774 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-11 18:49:24,775 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-11 18:49:24,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=bb921c4e-2a2d-4f68-a9b7-35e17bf11977
2018-01-11 18:49:24,921 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-7db69c5d-4796-444d-88f0-25ee3c547483
2018-01-11 18:49:24,921 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-11 18:49:24,929 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-9f9e4819-9863-4a6c-9ad1-cfd0eedcc866
2018-01-11 18:49:24,929 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-11 18:49:24,935 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-11 18:49:25,071 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-11 18:49:25,071 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 18:49:25,093 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-11 18:49:25,116 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-11 18:49:25,439 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 323ms
2018-01-11 18:49:25,522 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 407ms
2018-01-11 18:49:25,524 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 453ms
2018-01-11 18:49:25,538 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-11 18:49:25,538 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-11 18:49:25,538 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-11 18:49:25,538 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-11 18:49:25,635 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 97ms
2018-01-11 18:49:25,637 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 99ms
2018-01-11 18:49:25,648 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 111ms
2018-01-11 18:49:25,762 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-7db69c5d-4796-444d-88f0-25ee3c547483): no suitable block pools found to scan.  Waiting 1789263299 ms.
2018-01-11 18:49:25,762 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-9f9e4819-9863-4a6c-9ad1-cfd0eedcc866): no suitable block pools found to scan.  Waiting 1789263300 ms.
2018-01-11 18:49:25,827 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/11/18 9:40 PM with interval of 21600000ms
2018-01-11 18:49:25,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid bb921c4e-2a2d-4f68-a9b7-35e17bf11977) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-11 18:49:26,024 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid bb921c4e-2a2d-4f68-a9b7-35e17bf11977) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-11 18:49:26,025 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-11 18:49:26,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xb18ef9d3d3ceb3b7,  containing 2 storage report(s), of which we sent 2. The reports had 30 total blocks and used 1 RPC(s). This took 7 msec to generate and 72 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-11 18:49:26,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 19:00:31,252 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1619ms
No GCs detected
2018-01-11 19:01:57,348 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741914_1090 src: /192.168.28.132:33026 dest: /192.168.28.134:50010
2018-01-11 19:01:58,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:33026, dest: /192.168.28.134:50010, bytes: 63213, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1262417118_99, offset: 0, srvID: bb921c4e-2a2d-4f68-a9b7-35e17bf11977, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741914_1090, duration: 719387309
2018-01-11 19:01:58,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741914_1090, type=LAST_IN_PIPELINE terminating
2018-01-11 19:02:05,545 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741913_1089 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741913 for deletion
2018-01-11 19:02:05,548 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741913_1089 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741913
2018-01-11 19:04:48,145 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741914_1090 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741914 for deletion
2018-01-11 19:04:48,148 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741914_1090 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741914
2018-01-11 19:08:14,607 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741915_1091 src: /192.168.28.130:41840 dest: /192.168.28.134:50010
2018-01-11 19:08:14,687 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:41840, dest: /192.168.28.134:50010, bytes: 63213, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1353133492_145, offset: 0, srvID: bb921c4e-2a2d-4f68-a9b7-35e17bf11977, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741915_1091, duration: 27651967
2018-01-11 19:08:14,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741915_1091, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[192.168.28.132:50010, 192.168.28.131:50010] terminating
2018-01-11 19:10:44,542 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1241ms
No GCs detected
2018-01-11 19:11:38,149 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741916_1092 src: /192.168.28.130:41988 dest: /192.168.28.134:50010
2018-01-11 19:11:44,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:41988, dest: /192.168.28.134:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2117759426_12, offset: 0, srvID: bb921c4e-2a2d-4f68-a9b7-35e17bf11977, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741916_1092, duration: 6552188211
2018-01-11 19:11:44,715 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741916_1092, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[192.168.28.131:50010, 192.168.28.132:50010] terminating
2018-01-11 19:11:44,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741917_1093 src: /192.168.28.132:33030 dest: /192.168.28.134:50010
2018-01-11 19:11:49,185 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:33030, dest: /192.168.28.134:50010, bytes: 75090821, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2117759426_12, offset: 0, srvID: bb921c4e-2a2d-4f68-a9b7-35e17bf11977, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741917_1093, duration: 4428554828
2018-01-11 19:11:49,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741917_1093, type=LAST_IN_PIPELINE terminating
2018-01-11 19:11:49,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741918_1094 src: /192.168.28.130:41994 dest: /192.168.28.134:50010
2018-01-11 19:11:49,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:41994, dest: /192.168.28.134:50010, bytes: 66113, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2117759426_12, offset: 0, srvID: bb921c4e-2a2d-4f68-a9b7-35e17bf11977, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741918_1094, duration: 16682073
2018-01-11 19:11:49,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741918_1094, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[192.168.28.131:50010, 192.168.28.132:50010] terminating
2018-01-11 19:24:19,093 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741916_1092 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741916 for deletion
2018-01-11 19:24:19,096 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741917_1093 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741917 for deletion
2018-01-11 19:24:19,098 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741918_1094 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741918 for deletion
2018-01-11 19:24:19,099 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741916_1092 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741916
2018-01-11 19:24:19,100 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741918_1094 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741918
2018-01-11 19:24:19,100 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741917_1093 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741917
2018-01-11 19:26:40,956 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741919_1095 src: /192.168.28.132:33056 dest: /192.168.28.134:50010
2018-01-11 19:26:46,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:33056, dest: /192.168.28.134:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-780261031_12, offset: 0, srvID: bb921c4e-2a2d-4f68-a9b7-35e17bf11977, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741919_1095, duration: 5908222317
2018-01-11 19:26:46,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741919_1095, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-11 19:26:46,960 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741920_1096 src: /192.168.28.130:42258 dest: /192.168.28.134:50010
2018-01-11 19:26:50,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:42258, dest: /192.168.28.134:50010, bytes: 75090821, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-780261031_12, offset: 0, srvID: bb921c4e-2a2d-4f68-a9b7-35e17bf11977, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741920_1096, duration: 3722271237
2018-01-11 19:26:50,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741920_1096, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[192.168.28.132:50010, 192.168.28.131:50010] terminating
2018-01-11 19:26:51,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741921_1097 src: /192.168.28.131:59864 dest: /192.168.28.134:50010
2018-01-11 19:26:51,140 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:59864, dest: /192.168.28.134:50010, bytes: 66113, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-780261031_12, offset: 0, srvID: bb921c4e-2a2d-4f68-a9b7-35e17bf11977, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741921_1097, duration: 9266049
2018-01-11 19:26:51,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741921_1097, type=LAST_IN_PIPELINE terminating
2018-01-11 19:36:39,248 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2808ms
No GCs detected
2018-01-11 19:42:35,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xb18ef9d3d3ceb3b8,  containing 2 storage report(s), of which we sent 2. The reports had 33 total blocks and used 1 RPC(s). This took 2 msec to generate and 13 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-11 19:42:35,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 19:42:47,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741920_1096 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741920 for deletion
2018-01-11 19:42:47,082 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741921_1097 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741921 for deletion
2018-01-11 19:42:47,084 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741919_1095 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741919 for deletion
2018-01-11 19:42:47,085 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741920_1096 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741920
2018-01-11 19:42:47,085 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741921_1097 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741921
2018-01-11 19:42:47,087 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741919_1095 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741919
2018-01-11 19:44:02,395 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741922_1098 src: /192.168.28.131:59886 dest: /192.168.28.134:50010
2018-01-11 19:44:08,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:59886, dest: /192.168.28.134:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-531081096_12, offset: 0, srvID: bb921c4e-2a2d-4f68-a9b7-35e17bf11977, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741922_1098, duration: 5818050802
2018-01-11 19:44:08,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741922_1098, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-11 19:44:08,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741923_1099 src: /192.168.28.132:33062 dest: /192.168.28.134:50010
2018-01-11 19:44:10,913 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:33062, dest: /192.168.28.134:50010, bytes: 75090821, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-531081096_12, offset: 0, srvID: bb921c4e-2a2d-4f68-a9b7-35e17bf11977, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741923_1099, duration: 2611050728
2018-01-11 19:44:10,913 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741923_1099, type=LAST_IN_PIPELINE terminating
2018-01-11 19:44:11,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741924_1100 src: /192.168.28.131:59890 dest: /192.168.28.134:50010
2018-01-11 19:44:11,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:59890, dest: /192.168.28.134:50010, bytes: 66113, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-531081096_12, offset: 0, srvID: bb921c4e-2a2d-4f68-a9b7-35e17bf11977, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741924_1100, duration: 11121010
2018-01-11 19:44:11,244 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741924_1100, type=LAST_IN_PIPELINE terminating
2018-01-11 21:04:29,998 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2183ms
No GCs detected
2018-01-11 21:04:32,344 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker03.local/192.168.28.134"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-11 21:04:36,327 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-11 21:04:37,126 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-11 21:04:37,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker03.local/192.168.28.134
************************************************************/
2018-01-25 15:34:20,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker03.local/192.168.28.134
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-25 15:34:21,025 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-25 15:34:27,102 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-25 15:34:27,818 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-25 15:34:27,819 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-25 15:34:27,841 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-25 15:34:27,848 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker03.local
2018-01-25 15:34:27,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-25 15:34:27,975 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-25 15:34:27,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-25 15:34:27,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-25 15:34:28,892 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-25 15:34:29,020 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-25 15:34:29,088 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-25 15:34:29,114 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-25 15:34:29,117 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-25 15:34:29,118 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-25 15:34:29,118 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-25 15:34:29,182 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 34916
2018-01-25 15:34:29,183 INFO org.mortbay.log: jetty-6.1.26
2018-01-25 15:34:30,040 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:34916
2018-01-25 15:34:31,238 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-25 15:34:31,253 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-25 15:34:32,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-25 15:34:32,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-25 15:34:32,634 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-25 15:34:32,787 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-25 15:34:33,037 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-25 15:34:33,075 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-25 15:34:33,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-25 15:34:33,253 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-25 15:34:33,332 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-25 15:34:33,333 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-25 15:34:35,624 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-25 15:34:35,695 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-25 15:34:35,877 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 1801@hadoop-worker03.local
2018-01-25 15:34:36,001 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 1801@hadoop-worker03.local
2018-01-25 15:34:36,440 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-25 15:34:36,440 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-25 15:34:37,150 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-25 15:34:37,150 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-25 15:34:37,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=bb921c4e-2a2d-4f68-a9b7-35e17bf11977
2018-01-25 15:34:38,026 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-7db69c5d-4796-444d-88f0-25ee3c547483
2018-01-25 15:34:38,029 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-25 15:34:38,044 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-9f9e4819-9863-4a6c-9ad1-cfd0eedcc866
2018-01-25 15:34:38,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-25 15:34:38,506 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-25 15:34:38,621 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-25 15:34:38,621 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-25 15:34:38,630 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-25 15:34:38,626 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-25 15:34:39,154 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 452ms
2018-01-25 15:34:39,171 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 458ms
2018-01-25 15:34:39,186 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 565ms
2018-01-25 15:34:39,201 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-25 15:34:39,202 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-25 15:34:39,277 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-25 15:34:39,277 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-25 15:34:39,383 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 106ms
2018-01-25 15:34:39,521 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 319ms
2018-01-25 15:34:39,521 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 321ms
2018-01-25 15:34:39,800 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-7db69c5d-4796-444d-88f0-25ee3c547483): no suitable block pools found to scan.  Waiting 591349262 ms.
2018-01-25 15:34:39,815 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-9f9e4819-9863-4a6c-9ad1-cfd0eedcc866): no suitable block pools found to scan.  Waiting 591349247 ms.
2018-01-25 15:34:39,974 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/25/18 8:57 PM with interval of 21600000ms
2018-01-25 15:34:40,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid bb921c4e-2a2d-4f68-a9b7-35e17bf11977) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-25 15:34:40,903 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid bb921c4e-2a2d-4f68-a9b7-35e17bf11977) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-25 15:34:40,903 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-25 15:34:42,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x84e7a64c462a34f6,  containing 2 storage report(s), of which we sent 2. The reports had 33 total blocks and used 1 RPC(s). This took 14 msec to generate and 602 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-25 15:34:42,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-25 15:35:20,816 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2187ms
No GCs detected
2018-01-25 15:35:26,776 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1327ms
No GCs detected
2018-01-25 16:42:14,724 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741925_1101 src: /192.168.28.132:35930 dest: /192.168.28.134:50010
2018-01-25 16:42:26,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:35930, dest: /192.168.28.134:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-376932983_1, offset: 0, srvID: bb921c4e-2a2d-4f68-a9b7-35e17bf11977, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741925_1101, duration: 11885818202
2018-01-25 16:42:26,689 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741925_1101, type=LAST_IN_PIPELINE terminating
2018-01-25 16:42:26,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741926_1102 src: /192.168.28.131:50558 dest: /192.168.28.134:50010
2018-01-25 16:42:34,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:50558, dest: /192.168.28.134:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-376932983_1, offset: 0, srvID: bb921c4e-2a2d-4f68-a9b7-35e17bf11977, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741926_1102, duration: 7660877923
2018-01-25 16:42:34,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741926_1102, type=LAST_IN_PIPELINE terminating
2018-01-25 16:42:34,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741927_1103 src: /192.168.28.131:50560 dest: /192.168.28.134:50010
2018-01-25 16:42:40,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:50560, dest: /192.168.28.134:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-376932983_1, offset: 0, srvID: bb921c4e-2a2d-4f68-a9b7-35e17bf11977, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741927_1103, duration: 5953875504
2018-01-25 16:42:40,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741927_1103, type=LAST_IN_PIPELINE terminating
2018-01-25 16:42:40,419 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741928_1104 src: /192.168.28.130:53236 dest: /192.168.28.134:50010
2018-01-25 16:42:45,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:53236, dest: /192.168.28.134:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-376932983_1, offset: 0, srvID: bb921c4e-2a2d-4f68-a9b7-35e17bf11977, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741928_1104, duration: 4894802011
2018-01-25 16:42:45,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741928_1104, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[192.168.28.132:50010, 192.168.28.131:50010] terminating
2018-01-25 16:42:45,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741929_1105 src: /192.168.28.131:50562 dest: /192.168.28.134:50010
2018-01-25 16:42:50,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:50562, dest: /192.168.28.134:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-376932983_1, offset: 0, srvID: bb921c4e-2a2d-4f68-a9b7-35e17bf11977, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741929_1105, duration: 4719678743
2018-01-25 16:42:50,137 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741929_1105, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-25 16:42:50,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741930_1106 src: /192.168.28.132:35938 dest: /192.168.28.134:50010
2018-01-25 16:42:50,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:35938, dest: /192.168.28.134:50010, bytes: 18324704, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-376932983_1, offset: 0, srvID: bb921c4e-2a2d-4f68-a9b7-35e17bf11977, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741930_1106, duration: 443766004
2018-01-25 16:42:50,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741930_1106, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-25 16:52:00,928 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741931_1107 src: /192.168.28.132:35940 dest: /192.168.28.134:50010
2018-01-25 16:52:00,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:35940, dest: /192.168.28.134:50010, bytes: 2142, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-116525795_1, offset: 0, srvID: bb921c4e-2a2d-4f68-a9b7-35e17bf11977, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741931_1107, duration: 31862085
2018-01-25 16:52:00,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741931_1107, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-25 16:52:42,615 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741931_1107 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741931 for deletion
2018-01-25 16:52:42,618 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741931_1107 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741931
2018-01-25 16:56:28,487 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741932_1108 src: /192.168.28.130:53308 dest: /192.168.28.134:50010
2018-01-25 16:56:28,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:53308, dest: /192.168.28.134:50010, bytes: 82492, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-944352120_1, offset: 0, srvID: bb921c4e-2a2d-4f68-a9b7-35e17bf11977, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741932_1108, duration: 44863777
2018-01-25 16:56:28,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741932_1108, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[192.168.28.132:50010, 192.168.28.131:50010] terminating
2018-01-25 17:35:40,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741933_1109 src: /192.168.28.132:35944 dest: /192.168.28.134:50010
2018-01-25 17:35:47,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:35944, dest: /192.168.28.134:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_103600222_12, offset: 0, srvID: bb921c4e-2a2d-4f68-a9b7-35e17bf11977, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741933_1109, duration: 6840481707
2018-01-25 17:35:47,230 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741933_1109, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.131:50010] terminating
2018-01-25 17:35:47,316 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741934_1110 src: /192.168.28.131:50564 dest: /192.168.28.134:50010
2018-01-25 17:35:51,019 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:50564, dest: /192.168.28.134:50010, bytes: 75090816, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_103600222_12, offset: 0, srvID: bb921c4e-2a2d-4f68-a9b7-35e17bf11977, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741934_1110, duration: 3692432126
2018-01-25 17:35:51,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741934_1110, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-25 17:35:51,317 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741935_1111 src: /192.168.28.131:50566 dest: /192.168.28.134:50010
2018-01-25 17:35:51,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:50566, dest: /192.168.28.134:50010, bytes: 66113, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_103600222_12, offset: 0, srvID: bb921c4e-2a2d-4f68-a9b7-35e17bf11977, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741935_1111, duration: 11635765
2018-01-25 17:35:51,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741935_1111, type=LAST_IN_PIPELINE terminating
2018-01-25 17:37:59,952 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741933_1109 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741933 for deletion
2018-01-25 17:37:59,953 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741934_1110 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741934 for deletion
2018-01-25 17:37:59,956 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741933_1109 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741933
2018-01-25 17:37:59,959 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741935_1111 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741935 for deletion
2018-01-25 17:37:59,960 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741934_1110 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741934
2018-01-25 17:37:59,962 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741935_1111 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741935
2018-01-25 17:48:03,801 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: "hadoop-worker03.local/192.168.28.134"; destination host is: "hadoop-master":8020; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:554)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-25 17:48:07,433 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-25 17:48:08,436 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-25 17:48:08,997 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-25 17:48:09,023 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker03.local/192.168.28.134
************************************************************/
